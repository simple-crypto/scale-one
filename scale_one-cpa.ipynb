{"cells": [{"cell_type": "markdown", "id": "d2e069d9-cde4-470e-ad91-9accdd14043a", "metadata": {}, "source": ["# Appetizer: Textbook Correlation Power Analysis (CPA)\n", "\n", "In this section, we dig a bit more into side-channel analysis by implementing a more advanced attack. The aim of the first exercise is to carry out the example attack presented in Chapter 1 of the book, commonly known as CPA. next, we will dig a bit more into the impact of noise that we briefly encountered in the previous session.\n", "\n", "\n"]}, {"cell_type": "markdown", "id": "d8777c27-c9cd-4fa1-993a-e5ddbd379e2a", "metadata": {}, "source": ["## Correlation Power Analysis step-by-step\n", "As a reminder, this attack is a method that exploits the correlation between the practical measurements collected from an implementation execution and a model of these measurements. In order to perform a CPA, you generally use an a-priori leakage model and assumes that the power consumption of the target device is linearly correlated with it. \n", "\n", "To exploit such a model, Pearson\u2019s correlation coefficient is a natural candidate. This statistic measures how well the leakages can be predicted by a given linear model. The underlying idea is that the subkey hypothesis predicting the leakages best should be the correct one. \n", "\n", "The following exercises will guide you through implementing a CPA from scratch."]}, {"cell_type": "markdown", "id": "8bdb9262-ac2a-4159-8c27-4dd311ed3f3e", "metadata": {}, "source": ["### Step 1: Computation of the correlation\n", "The first step of this exercise is to implement the computation of the correlation between measurements and their model, as defined below:\n", "\n", "$$r_{xy} = \\dfrac{\\mathsf{cov(X,Y)}}{\\sigma_X \\sigma_Y}=\\dfrac{ \\mathsf{\\hat E}(X^c Y^c)}{\\sqrt{\\mathsf{\\hat V}(X)\\mathsf{\\hat V}(Y)}}$$\n", "\n", "where $\\mathsf{\\hat E}$ and $\\mathsf{\\hat V}$ denote respectively the empirical mean and variance, $ X $ represents the leakage values, and $ Y $ represents the model values (e.g., the Hamming weight of the hypothetically manipulated values), while $X^c = X - \\mathsf{\\hat E}(X)$ (and similarly for $Y^c$).\n", "\n", "Without knowing a priori exactly where the interesting operations are located in the traces, we have to compute the correlation that exists between the model associated to the target intermediate state and all the time samples in the traces. Although this can be done in a for iteration loop over all sample location in a trace, we invite you to try to carry out the operations in the form of vector/matrix operations in order to obtain good performance (This advice applies to the rest of the training too). \n", "\n", "You first task in order to compute the correlation is to compute the covariance by yourself. For that, we invite you to implement the following function:\n", "\n", "- *hint: NumPy [mean](https://numpy.org/doc/2.2/reference/generated/numpy.mean.html) and [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html) may be useful*"]}, {"cell_type": "code", "execution_count": 1, "id": "c866f366-241f-4747-9e4c-ec24472f2e6a", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "#TODO implement this function\n", "def covariance(l, m):\n", "    \"\"\"\n", "    Compute the covariance metric between the leakage trace \n", "    and the modelled leakage values, for each independant time index of the traces.\n", "    \n", "    l: the raw traces, as an NumPy array of shape (ntraces, nsamples)\n", "    m: the modelled values, as a NumPy array of shape (ntraces, nsamples)\n", "\n", "    Return: the covariance as the NumPy array 'cov' of shape (nsamples,)\n", "    such that cov[i] is the covariance between l[:,i] and m[:]\n", "    \"\"\"\n", "    "]}, {"cell_type": "markdown", "id": "ccfab6c0-8393-412c-bc93-7798e8192c6a", "metadata": {}, "source": ["You can verify your implementation here:"]}, {"cell_type": "code", "execution_count": 2, "id": "758f686c-68e5-4951-9a0f-f7569cf3bf12", "metadata": {}, "outputs": [], "source": ["from utils_scale import test_scale\n", "test_scale.test_covariance(covariance)"]}, {"cell_type": "markdown", "id": "3a6da03c-d87f-4fce-819b-7ffbb7050135", "metadata": {}, "source": ["The next step is to compute the standard deviation of the different variable. To do so, you asked to implement the following function\n", "\n", "- *hint: see NumPy [std](https://numpy.org/doc/stable/reference/generated/numpy.std.html)*"]}, {"cell_type": "code", "execution_count": 3, "id": "a6e9501f-ecf7-4e54-bd80-12876817976f", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "#TODO implement this function\n", "def std(v):\n", "    \"\"\"\n", "    Compute the column-wise standard deviation, where a \n", "    single column contains different outcomes of a single random variable. \n", "    \n", "    v: the variables, as an NumPy array of shape (nexec, nvars)\n", "\n", "    Return: the column-wise standard deviation as the NumPy array 'std' of shape (nvars,) \n", "    \"\"\"\n"]}, {"cell_type": "markdown", "id": "0f52d91c-6b27-4839-8d80-3e1067b29cd1", "metadata": {}, "source": ["You can verify your implementation here:"]}, {"cell_type": "code", "execution_count": 4, "id": "96fa51be-affd-40c8-8b53-dba112ab64cb", "metadata": {}, "outputs": [], "source": ["from utils_scale import test_scale\n", "test_scale.test_std(std)"]}, {"cell_type": "markdown", "id": "374b1210-4245-4692-a1bb-26e69f15e656", "metadata": {}, "source": ["As a final step, you are invited to use the function `covariance` and `std` that you just implemented in order to implement the function `pearson_corr` that computes the Pearson Correlation! "]}, {"cell_type": "code", "execution_count": 5, "id": "0c4317a2-0a2b-46ea-88e6-3dcedddcd286", "metadata": {}, "outputs": [], "source": ["def pearson_corr(l, m):\n", "    \"\"\"\n", "    Compute the pearson column-wise correlation between the \n", "    leakage trace and their model. \n", "\n", "    l: the leakage matrix of shape (ntraces, nsamples)\n", "    m: the model matrix of shape (ntraces, nsamples)\n", "\n", "    Return: the column-wise correlation between the matrices 'l' and 'm', as the NumPy array 'corr' of shape (nsamples,)\n", "    such that corr[i] is the correlation between l[:,i] and m[:] \n", "    \"\"\"\n"]}, {"cell_type": "markdown", "id": "28ec5bb9-99df-4e84-b8f8-722e46263e56", "metadata": {}, "source": ["You can verify your implementation here:"]}, {"cell_type": "code", "execution_count": 6, "id": "355c5043-8e55-4637-b021-ef0a8f2afd08", "metadata": {}, "outputs": [], "source": ["from utils_scale import test_scale\n", "test_scale.test_pearson_corr(pearson_corr)"]}, {"cell_type": "markdown", "id": "b1866090-e7af-4e42-afc1-5f9b9c97245e", "metadata": {}, "source": ["### Step 2: Model computation.\n", "\n", "A simple model proposed is the Hamming Weight model, which represents the leakage as the sum of the bits set to '1' for the values taken by a specific intermediate variable manipulated by the implementation (e.g., the values at the output of the S-box). This hypothesis is commonly found in the literature and is a reasonable approximation observed in practice for CMOS technology implementations . Don't hesitate to go back to the previous session in order to compare the trace behavior associated to different value of a single Sbox output if you want to see for yourself!\n", "\n", "To make things easier for you, we've compiled the hamming weigh associated with the 256 possible byte values in the form of a NumPy array. The following code shows you how to use it:"]}, {"cell_type": "code", "execution_count": 7, "id": "879d1859-8b94-470d-ae01-9f4228d08ccc", "metadata": {}, "outputs": [], "source": ["from utils_scale.utils_aes import HW\n", "\n", "# Similarly to the 'Sbox' array that you used before, 'HW' is an array\n", "# such that Sbox[i] is the Hamming Weight of the byte value 'b'.\n", "\n", "# Hamming Weight example\n", "HW_0xAA = HW[0xAA]\n", "HW_0x00 = HW[0x00]\n", "HW_0xFF = HW[0xFF]\n", "assert HW_0xAA == 4, \"The HW 0xAA must be equal to 4\"\n", "assert HW_0x00 == 0, \"The HW 0xAA must be equal to 0\"\n", "assert HW_0xFF == 8, \"The HW 0xAA must be equal to 8\"\n", "print(\"You know now how to use the 'HW' array :)\")"]}, {"cell_type": "markdown", "id": "4cf5a481-eccf-450a-af61-59e341d966f5", "metadata": {}, "source": ["Now that we've chosen which model to use, we need to define which intermediate variable we want to target. To continue with what you've already had a chance to manipulate, we propose to model the leakage associated with Sbox output values. To keep it simple, we will focus on a 8-bit state only first (out of 128). \n", "\n", "Your next goal is to compute the model for the byte we target. To do so, you have to implement the followign function"]}, {"cell_type": "code", "execution_count": 8, "id": "543a7c2f-49e5-45d2-9d5c-6ff008a39c8f", "metadata": {}, "outputs": [], "source": ["from utils_scale.utils_aes import Sbox, HW\n", "\n", "#TODO implement this function\n", "def HW_sbox_output(pts, ks):\n", "    \"\"\"\n", "    Compute the Hamming Weight of the single byte resulting from the Sbox operation, for each execution.\n", "\n", "    pts: the plaintext bytes values for the targeted byte index, as an array of shape (nexec, ) and of type np.uint8\n", "    ks: the key bytes values for the targeted byte index, as an array of shape (nexec,) and if type np.uint8\n", "\n", "    Returns: the Hamming weigh of the value at the output of the Sbox, as an array of shape (nexec,)\n", "    \"\"\"\n"]}, {"cell_type": "markdown", "id": "fa11a661-53b9-41cb-bd4b-e0adb7e0df8d", "metadata": {}, "source": ["You can verify you function here:"]}, {"cell_type": "code", "execution_count": 9, "id": "2955c4ee-2ff1-49e6-9f9e-391d308caa85", "metadata": {}, "outputs": [], "source": ["from utils_scale import test_scale\n", "test_scale.test_HW_sbox_output(HW_sbox_output)"]}, {"cell_type": "markdown", "id": "c8d89772-a156-488f-9c88-11b58049ded0", "metadata": {}, "source": ["### Step 3: Compute the correlation with real measurements\n", "\n", "Things are starting to get interesting: your next goal is to compute the correlation you get for real traces! To this purpose, you have to implement the following function\n", "\n", "- *hint: you are targeting a single intermediate state, but you have to provide a model for each time samples.*\n", "- *hint2: see [tile](cpa_out_sbox_profiled)*"]}, {"cell_type": "code", "execution_count": 10, "id": "cd3514b5-fb20-4e45-a55c-51ab4ca78e2a", "metadata": {}, "outputs": [], "source": ["# TODO: implement this function\n", "def corr_HW_traces_outSB(traces, pts, ks):\n", "    \"\"\"\n", "    Compute the correlation between the traces and the HW model of a single Sbox output, for every time samples.\n", "\n", "    traces: the traces, as an array of shape (nexec, nsamples)\n", "    pts: the plaintext byte value targeted, as an array of shape (nexec,)\n", "    ks: the key byte value target, as an array of shape (nexec,)\n", "\n", "    Return, the correlation, as an array of shape (nsamples, )\n", "    \"\"\"\n"]}, {"cell_type": "markdown", "id": "2805fe6e-1c80-4225-89b3-1b18a2b8de5d", "metadata": {}, "source": ["You can verify your implementation here:"]}, {"cell_type": "code", "execution_count": 11, "id": "63ca33e1-eefb-4ec4-bc37-eed436dea21e", "metadata": {}, "outputs": [], "source": ["from utils_scale import test_scale\n", "\n", "test_scale.test_corr_HW_traces_outSB(corr_HW_traces_outSB)"]}, {"cell_type": "markdown", "id": "ad3817c5-7f78-4d39-8a97-adfd6d170ecf", "metadata": {}, "source": ["In order to gain some intuition about the correlation you compute. Assuming the hypothesis holds, a correlation peak should appear at the time sample corresponding to the moment when the targeted intermediate value is actually manipulated by the device. See for yourself with the following template that allows you to plot the correlation computed for a specific byte index (you can tune which one by adjusting the variable `bindex`). Is it coherent with what you observed in the previous session? \n", "\n", "Alternatively, try to used wrong intermediate variables values used to compute the model (e.g., set all the plaintext bytes to 0). \n", " What do you observe in such a situation? How do you explain it? "]}, {"cell_type": "code", "execution_count": 12, "id": "e71aa5e3-adbe-4b1a-83cf-2eaea6d22861", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_plot, utils_obs\n", "%matplotlib widget\n", "import matplotlib.pyplot as plt\n", "\n", "# TODO: try to see what happens when you change this parameter\n", "# Targeted byte index\n", "bindex = 0\n", "\n", "# Load the dataset with all random inputs\n", "dataset = utils_obs.load_dataset_all_random()\n", "\n", "# Compute the correlation associated to the targeted byte index\n", "corr = corr_HW_traces_outSB(\n", "    dataset['traces'], \n", "    dataset['pts'][:,bindex], # TODO, modify HERE \n", "    dataset['ks'][:, bindex]\n", ")\n", "\n", "# Display the correlation, together with an examplary trace\n", "utils_plot.display_correlation(corr, dataset['traces'][0,:], bindex)"]}, {"cell_type": "markdown", "id": "6b9cfcf3-75e4-4fcc-82d3-5bfe6436c6ee", "metadata": {}, "source": ["### Step 4: Attacking a single key byte.\n", "\n", "At last, here is what you've probably been waiting for: implementing the attack in itself! In the context of a practical attack, the adversary is considered to have access to a set of traces as well as the associated plaintext that have been used. His goal is the recover the value of the fixed (and unknown) key used to process the plaintexts. \n", "\n", "The most common CPA exploits leakages from intermediate values manipulated by the cryptographic implementation, whose value depends on both the key and known plaintexts (see KIS sensitive variable, Definition 3 in the book). As shown in the previous step, the values manipulated after the substitution layer appear to be a good target because of the significant levels of correlation that they enable. The attack underlying idea is that the subkey hypothesis that predicts the leakages best should be the correct one. Put in another way, the correlation computed should be maximum for the correct subkey, and it should be significantly lower for the others (as shown with the `mask` parameter above).\n", "\n", "However, making hypotheses about all the key bytes simultaneously can be computationally difficult: it would imply to enumerate $2^{128}$ different hypotheses, which is far from reachable in practice nowadays. For this reason, a commonly used strategy is to follow a divide and conquer approach. The main idea of the latter is to attack (small) independent sub-parts of the key independently, and combine the results obtained together. \n", "\n", "In this exercise, we propose to follow such a strategy by targeting each byte of the key sequentially, and to combine the results obtained for the 16 independant attacks together to recover the full 128-bit key. To this end, the basic methodology can be summarized as follows:\n", "\n", "\n", "+ For each key byte index $i$:\n", "    + For each key candidate $k_{i}^* \\in [0; 255]$:\n", "        + Predict the intermediate values that are expected to be manipulated at each execution under the hypothesis that $k_{i}^*$ is used:\n", "            + $x_{i} = p_i \\oplus k_{i}^*$ when using the S-box input\n", "            + $y_i = \\text{Sbox}(x_i)$ when using the S-box output\n", "        + Compute the Hamming Weight of the intermediate values, considered as the leakage model.\n", "        + Compute the correlation between the model and the practical measurements provided.\n", "    + Keep the value $k_{i}^*$ maximising the correlation as the predicted correct key byte.\n", " \n", "To put this methodology into practice, we first ask you to perform the CPA using the leakage of the Sboxes output against a single byte by implementing the function `cpa_byte_out_sbox`. \n", "\n", "- *hint: see [zeros](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html#numpy.zeros), [abs](https://numpy.org/doc/stable/reference/generated/numpy.absolute.html), [max](https://numpy.org/doc/stable/user/basics.broadcasting.html), [argsort](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html#numpy-argsort), and [flip](https://numpy.org/doc/stable/reference/generated/numpy.flip.html#numpy.flip) from NumPy*\n", "- *hint2: Do you have to consider the complete traces in your computations? From what you saw in the previous exercice, you may want keep only a smaller part of these in order to speed up your computations.*"]}, {"cell_type": "code", "execution_count": 13, "id": "94ec7723-5725-4bf4-8436-47281bb803e3", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "#TODO implement this function\n", "def cpa_byte_out_sbox(traces, pts):\n", "    \"\"\" \n", "    Perform a CPA against a single key byte.\n", "\n", "    traces: the traces, as an array of shape (nexec, nsamples)\n", "    pts: the plaintext bytes associated to the target key byte, as an arrya of shape (nexec, )\n", "\n", "    Returns: a NumPy array of shape (256,), with the 256 key bytes candidates ordered \n", "    from the highest correlation to the less one (the candidat.\n", "    \"\"\"\n"]}, {"cell_type": "markdown", "id": "ee239c79-e988-4a32-b831-b419432f0b1a", "metadata": {}, "source": ["To verify your implementation, lets try to perform the attack against a real dataset! In particular, you can use the following template in order to run your attack against a fixed-key dataset. You can define the amount of traces used to perform the attack (i.e., the *attack data complexity*) by setting the value of the variable `q_a`. How is this parameter affecting the success of you attack? What is the minimal amount of traces required to recover the key byte? Is it the same for every byte index? \n", "\n", "- *NB: depending on your implementation, running the attack may take time to proceed when too much time are used.*"]}, {"cell_type": "code", "execution_count": 14, "id": "68b407f3-ed0b-4a09-9cdc-fe625e1e1fea", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_files\n", "\n", "# Load the fixed-key dataset\n", "dataset = utils_files.load_dataset(utils_files.VALIDATION_DS[0])\n", "\n", "# TODO: modify this parameter\n", "q_a = 100\n", "\n", "# TODO: modify this parameter\n", "bindex = 0 \n", "\n", "# Run the attack\n", "k_guesses = cpa_byte_out_sbox(dataset[\"traces\"][:q_a,:], dataset[\"pts\"][:q_a,bindex])\n", "\n", "if k_guesses[0]==dataset['ks'][0,bindex]:\n", "    print(f\"Nice job! The proper key byte ({dataset['ks'][0,bindex]}) has been recovered :) \")\n", "else:\n", "    print(f\"The value {k_guesses[0]} has been recovered instead of {dataset['ks'][0,bindex]} :(\")"]}, {"cell_type": "markdown", "id": "5f50a8eb-4b66-44ce-ac7b-0297eca2548a", "metadata": {}, "source": ["### Step 5: Attacking a full key"]}, {"cell_type": "markdown", "id": "dd3c2b2c-692d-48be-8233-aed7faa413da", "metadata": {}, "source": ["As the cherry on the top, the final step is to perform the CPA against the full key, by implementing the function `complete_cpa_out_sbox` defined next. How many traces are required to recover the full key? "]}, {"cell_type": "code", "execution_count": 15, "id": "42da7965-6690-4cf3-b833-172b5efe228d", "metadata": {}, "outputs": [], "source": ["# TODO: implement this function\n", "def complete_cpa_out_sbox(traces, pts):\n", "    \"\"\" \n", "    Perform a CPA against the full key.\n", "\n", "    traces: the traces, as an array of shape (nexec, nsamples)\n", "    pts: the plaintexts used, as an array of shape (nexec, 16)\n", "\n", "    Returns: a NumPy array of shape (16,), with the best key candidate \n", "    \"\"\"\n"]}, {"cell_type": "markdown", "id": "5f68c9ca-0c8e-472f-95c7-ccae24e421ee", "metadata": {}, "source": ["Run the full attack next:"]}, {"cell_type": "code", "execution_count": 16, "id": "2508348f-4c55-4291-a5c1-86b1b78a91ea", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_files\n", "\n", "# Trace cropping window to use\n", "# NB: you may want to change that\n", "start=0\n", "end=15000\n"]}, {"cell_type": "markdown", "id": "6ecde54b-7c95-473d-a1d4-1589a152d2ac", "metadata": {}, "source": ["### CPA: An efficient turnkey tool with SCALib\n", "\n", "Never heard about [SCALib](https://scalib.readthedocs.io/en/latest/index.html#)? The Side-Channel Analysis Library (SCALib) is a Python package that contains state-of-the-art tools for side-channel evaluation, focusing on efficiency and easy interface. It turns out to contain an implementation of [CPA](https://scalib.readthedocs.io/en/latest/source/api/scalib.attacks.Cpa.html#scalib.attacks.Cpa). \n", "\n", "We provide next the implmentation of the function `scalib_complete_cpa_out_sbox` that is an alternative to `complete_cpa_out_sbox` relying on SCALib! As a final exercice, try to understand how it works and compare the performances achieved between the functions `complete_cpa_out_sbox` and `scalib_complete_cpa_out_sbox`. You should normally see a significative speed improvement when processing higher number of traces :) \n", "\n", "- *NB: these two functions are available from `utils_scale.utils_cpa`*"]}, {"cell_type": "code", "execution_count": 17, "id": "782a5183-1db4-4fdf-b709-09b8ddcd8bfe", "metadata": {}, "outputs": [], "source": ["from utils_scale.utils_aes import HW\n", "from scalib.attacks import Cpa\n", "import numpy as np\n", "\n", "def scalib_corr_traces(traces, pts, models):\n", "    \"\"\" \n", "    Compute the HW correlation, for several variables.\n", "\n", "    traces: the traces, as an array of shape (nexec, nsamples)\n", "    pts: the plaintexts used, as an array of shape (nexec, nvars)\n", "    models: array of shape (nvars, nclasses, nsamples), \n", "\n", "    Returns: a NumPy array of shape (nvars,nclasses,nsamples), with the correlation values associated to each of the independent subkey candidate\n", "    \"\"\"\n", "    cpa = Cpa(nc=models.shape[1], kind=Cpa.Xor)\n", "    cpa.fit_u(np.round(traces).astype(np.int16),pts.astype(np.uint16))\n", "    return cpa.get_correlation(models)\n", "\n", "def model_HW_outSB(nvars, nsamples):\n", "    \"\"\"\n", "    Compute the HW models for SCALib CPA, for all the Sboxes output variables.\n", "\n", "    To speed up computation, the intermediate considered in SCALib CPA is the result of an configurable \n", "    operation (e.g. XOR) between the key and the states provided during the fitting phase. \n", "    In our case, the class label of the models are the one correspondign to the result of the XOR between \n", "    the plaintext byte and the candidate key byte, or the input of the Sbox. \n", "    \n", "    \"\"\"\n", "    return np.tile(HW[Sbox][np.newaxis,:,np.newaxis], (nvars, 1, nsamples)).astype(np.float64)\n", "\n", "def scalib_complete_cpa_out_sbox(traces, pts):\n", "    \"\"\" \n", "    Perform a CPA against the full key.\n", "\n", "    traces: the traces, as an array of shape (nexec, nsamples)\n", "    pts: the plaintexts used, as an array of shape (nexec, 16)\n", "\n", "    Returns: a NumPy array of shape (16,), with the best key candidate \n", "    \"\"\"\n", "    models = model_HW_outSB(pts.shape[1], traces.shape[1])\n", "    correlation_abs = np.abs(scalib_corr_traces(traces,pts, models))\n", "    return np.argmax(np.max(correlation_abs, axis=2), axis=1)\n", "    "]}, {"cell_type": "code", "execution_count": 18, "id": "2df54d8d-7bee-4477-a7e4-326329408683", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_files\n", "\n", "# Load the fixed-key dataset (no cropping here)\n", "dataset = utils_files.load_dataset(utils_files.VALIDATION_DS[0])\n", "\n", "# TODO: increse here and see how the performances compare to your implementation!\n", "q_a = 1000\n", "\n", "# Run the attack\n", "k_guess = scalib_complete_cpa_out_sbox(dataset['traces'][:q_a], dataset['pts'][:q_a])\n", "\n", "# Verify results:\n", "correct_key = dataset['ks'][0,:]\n", "for i, (e,ce) in enumerate(zip(k_guess,correct_key)):\n", "    print(f\"k[{i}] recovered is {e} (must be {ce}) -> {'SUCCESS' if e==ce else 'FAIL'}\")"]}, {"cell_type": "markdown", "id": "c028ad3e-58f1-49a8-a37a-518ff39cf31d", "metadata": {}, "source": ["# Impact of the noise on CPA"]}, {"cell_type": "markdown", "id": "28f23f84-c879-4e60-b222-469f92d09430", "metadata": {}, "source": ["In the following, we'll go into more detail about the noise we introduced in the first session. More specifically, we'll look at its impact on CPA performance, and we'll briefly touch on some sources of non-ideality commonly observed in real measurements.\n", "\n", "The following code snippet allows you to compute the correlation between an HW model (targetting the Sbox output) and traces. Howver, unlike what you've already done, it performs this computation on two different types of traces: (1) the original traces, and (2) a more noisy version of them. \n", "Here, we simulate in fact a physical noise level higher than the one affecting our practical measurements. We achieve this by adding to each sample of each trace a random value drawn from a Gaussian centered at zero, for which you can choose the standard deviation (by adjusting `noise_std`). Besides computing the correlation, you can also plot the results for both dataset by running an independant cell: both are volontarily splitted in order to easily display the results for different `bindex` without having to recompute all the correlation values. \n", "\n", "Try to observe the impact of noise. More particulalry:\n", "\n", "- Do you observe an impact on the traces (e.g., visually)?\n", "- Same question for the correlation value? How does it evolve with the noise level?\n", "  Is it the same for all the different bytes?\n", "- Based on this, what are your expectations regarding the required attack complexity to perform a CPA if the noise level increases?"]}, {"cell_type": "code", "execution_count": 31, "id": "70ee5392-ad73-440c-9157-fdfb7c54bb03", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_cpa, utils_files\n", "\n", "# standard deviation of the additive noise\n", "noise_std = 1000\n", "\n", "# Load the noisy version of the dataset\n", "dataset_noisy = utils_cpa.load_noisy_dataset(utils_files.VALIDATION_DS[0],std=noise_std)\n", "# Load the noise-free traces\n", "traces_origin = utils_cpa.load_noisy_dataset(utils_files.VALIDATION_DS[0],std=0.0)['traces']\n", "\n", "# Compute the correlation associated to both cases\n", "models = utils_cpa.model_HW_outSB(dataset_noisy['pts'].shape[1], traces_origin.shape[1])\n", "corr_noisy = utils_cpa.scalib_corr_traces(dataset_noisy['traces'], dataset_noisy['pts'], models)\n", "corr_origin= utils_cpa.scalib_corr_traces(traces_origin, dataset_noisy['pts'], models)"]}, {"cell_type": "code", "execution_count": 32, "id": "403de9d5-cd32-402a-b248-919faf0d8036", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_plot\n", "%matplotlib widget\n", "import matplotlib.pyplot as plt \n", "\n", "# Byte index to plot\n", "# TODO: modify this value\n", "bindex = 8\n", "\n", "# Plot the figure\n", "utils_plot.display_noisy_correlation(corr_noisy, corr_origin, dataset_noisy['traces'], traces_origin, dataset_noisy[\"ks\"][0], bindex, noise_std)"]}, {"cell_type": "markdown", "id": "8b34b3f7-b1ca-4d08-a092-03da253c3462", "metadata": {}, "source": ["The following code can be used to verify your hypotheses: in particular, it runs several CPA using different number of traces and different noise levels, and display the amount of bytes recovered for each case (i.e., each curve represents a different noise level). Does it corresponds to what you were expecting?"]}, {"cell_type": "code", "execution_count": 33, "id": "4f3132c6-6acc-4271-bfdb-f8e77da1e6b9", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_files, utils_cpa\n", "%matplotlib widget\n", "import matplotlib.pyplot as plt\n", "\n", "# Dataset to use\n", "ds_filepath = utils_files.VALIDATION_DS[0]\n", "\n", "# TODO: edit here\n", "# The standard deviation to use\n", "stds = [10, 100, 1000, 5000, 10000]\n", "\n", "# TODO: edit here\n", "# The attack complexity to use\n", "qas = [1, 4, 16, 32, 128, 512, 2048, 4096]\n", "\n", "utils_cpa.cpa_with_exploration(ds_filepath, utils_cpa.scalib_complete_cpa_out_sbox, std=stds, qa=qas)"]}, {"cell_type": "markdown", "id": "7c621459-8df6-4a95-8134-28199bb220a6", "metadata": {}, "source": ["How do you explain this phenomenon? Try to find a theoritical reason behind that. To get some intuition, the following script display the different numerical values involved in the computation of the correlation. \n", "\n", "- *hint: how do the correlation numerator and denominator evolve with the noise? Which one has a bigger impact? Is it expected?*"]}, {"cell_type": "code", "execution_count": 22, "id": "af92d015-3771-4497-9e73-05a5cc0fc0cf", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_files, utils_cpa\n", "%matplotlib widget\n", "import matplotlib.pyplot as plt\n", "\n", "# Dataset to use\n", "ds_filepath = utils_files.VALIDATION_DS[0]\n", "\n", "# TODO: edit here\n", "# The standard deviation to use\n", "stds = [0,10, 100, 1000, 5000, 10000]\n", "\n", "# TODO: edit here:\n", "bindex=8\n", "\n", "utils_cpa.cpa_parts_exploration(ds_filepath, bindex, std=stds)"]}, {"cell_type": "markdown", "id": "6bba6cf9-796e-4da5-a02f-a76c1cb03e04", "metadata": {}, "source": ["## Dataset quality and measurement setup engineering problems\n", "So far, we've seen the impact that Guassian noise can have on correlation values, and therefore on the complexity required for a successful CPA attack. This type of noise can typically be traced back to the additive white noise affecting any electronic device. \n", "\n", "Additionally, this section present other mechanism that may impact the quality of measurement, potentially making these more difficult (or infeasible) to exploit. In particular, you will manipulate different datasets that are affected with contaminations that typically occur with a lack of precaution or with a measurement setup of poor quality.\n", "\n", "To evaluate the effects more objectively, we suggest you compare the correlation metrics between a standard dataset and an uncontaminated dataset, as allowed by the `utils_cpa.scalib_corr_traces` function you've already used."]}, {"cell_type": "markdown", "id": "60a860f3-9762-4d62-8f17-5b2026ea9202", "metadata": {}, "source": ["### Clipping\n", "In this first case, we'll look at the effects of clipping. Clipping typically occurs as a result of a poorly configured vertical range (i.e., too small) of the oscilloscope, leading to a significant reduction in measurement resolution. As an example, consider a circuit with a current consumption such that the voltage across the measured shunt resistor oscillates between +-50mV. In this configuration, an oscilloscope configured with a range of +-20mV will saturate at the same value as soon as the instantaneous voltage exceeds the configured threshold. Saturated values can therefore no longer be used as a source of discriminating information, significantly (if not totally) reducing measurement quality.\n", "\n", "See for yourself! Try to run the following template and observe the effect of clipping. In particular:\n", "\n", "- What do you observe visually compared to a unaffected dataset?\n", "- How does it change when you modify `clip_mv`?\n", "- What is the impact on the correlation metric? What do you expect for the CPA attack complexity?\n", "- Would the conclusion be the same for other intermediate variables than the Sbox output? (*hint: consider other locations in the trace*)"]}, {"cell_type": "code", "execution_count": 42, "id": "d8515d35-9162-4809-9160-d4565f61dcb5", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_cpa, utils_files\n", "# range used when clipping occur (50mV used for a correct acquiision)\n", "# TODO: modify this value\n", "clip_mv = 20\n", "\n", "# Load the noisy version of the dataset\n", "dataset_noisy = utils_cpa.load_clipped_dataset(utils_files.VALIDATION_DS[0], mv_clip=clip_mv)\n", "# Load the noise-free traces\n", "traces_origin = utils_files.load_dataset(utils_files.VALIDATION_DS[0])['traces']\n", "\n", "# Compute the correlation associated to both cases\n", "models = utils_cpa.model_HW_outSB(dataset_noisy['pts'].shape[1], traces_origin.shape[1])\n", "corr_noisy = utils_cpa.scalib_corr_traces(dataset_noisy['traces'], dataset_noisy['pts'], models)\n", "corr_origin= utils_cpa.scalib_corr_traces(traces_origin, dataset_noisy['pts'], models)"]}, {"cell_type": "code", "execution_count": 43, "id": "a98360d5-75eb-4546-99dd-59081b2717f4", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_plot\n", "%matplotlib widget\n", "import matplotlib.pyplot as plt \n", "\n", "# Byte index to plot\n", "# TODO: modify this value\n", "bindex = 8\n", "\n", "# Plot the figure\n", "utils_plot.display_clipped_correlation(corr_noisy, corr_origin, dataset_noisy['traces'], traces_origin, dataset_noisy[\"ks\"][0], bindex, clip_mv)"]}, {"cell_type": "markdown", "id": "677d17aa-7595-43ee-b2ba-0a13df15e363", "metadata": {}, "source": ["### Desynchronization"]}, {"cell_type": "markdown", "id": "92f31915-ecc7-4d5f-ab14-dcd62eb3915a", "metadata": {}, "source": ["Trace desynchronization is an effect that occurs when the measurements of each execution are not aligned in time. Without going into too much detail, this kind of effect is often the result of a missing or incorrect trigger signal. In laboratories with a controlled environment, it is relatively easy to obtain a dedicated trigger signal. However, this is not necessarily the case in practical attack environments, where we are forced to rely on re-alignment euristics (such as maximum correlation with an identifiable pattern). However, the presence of a trigger signal does not guarantee that there is no misalignment: desynchronization remains possible, for example, if the phase shift between the target clock frequency and the sampling frequency varies. Generally speaking, it's good practice to use a sampling frequency that's a multiple of the measured clock frequency.\n", "\n", "See for yourself! Try to run the following template and observe the effect of de-sync. In particular:\n", "\n", "- What do you observe visually compared to a unaffected dataset?\n", "- What is the impact on the correlation metric? What do you expect for the CPA attack complexity?\n", "- Would the conclusion be the same for other intermediate variables than  the Sbox output? (*hint: consider other locations in the trace*)"]}, {"cell_type": "code", "execution_count": 25, "id": "0c4e7879-5a93-4312-af74-1dd2387192c5", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_cpa, utils_files\n", "# std of the misalignement (in amount of samples)\n", "# TODO: modify this value\n", "std_misalign = 5\n", "\n", "# Load the noisy version of the dataset\n", "dataset_noisy = utils_cpa.load_misalign_dataset(utils_files.VALIDATION_DS[0], std_mis=std_misalign)\n", "# Load the noise-free traces\n", "traces_origin = utils_cpa.load_misalign_dataset_ref(utils_files.VALIDATION_DS[0])['traces']\n", "\n", "# Compute the correlation associated to both cases\n", "models = utils_cpa.model_HW_outSB(dataset_noisy['pts'].shape[1], traces_origin.shape[1])\n", "corr_noisy = utils_cpa.scalib_corr_traces(dataset_noisy['traces'], dataset_noisy['pts'], models)\n", "corr_origin= utils_cpa.scalib_corr_traces(traces_origin, dataset_noisy['pts'], models)"]}, {"cell_type": "code", "execution_count": 26, "id": "9c3d1880-0d14-4041-a3fd-259fa644be58", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_plot\n", "%matplotlib widget\n", "import matplotlib.pyplot as plt \n", "\n", "# Byte index to plot\n", "# TODO: modify this value\n", "bindex = 8\n", "\n", "# Plot the figure\n", "utils_plot.display_misaligned_correlation(corr_noisy, corr_origin, dataset_noisy['traces'], traces_origin, dataset_noisy[\"ks\"][0], bindex, std_misalign)"]}, {"cell_type": "markdown", "id": "5ec77bd5-85ee-40f5-a7ee-d8bb38b45d33", "metadata": {}, "source": ["### A quiet threat: temporal average."]}, {"cell_type": "markdown", "id": "4c0b3ba9-a5c8-4c03-abf6-bf8d0e12cda1", "metadata": {}, "source": ["The last example we'll cover concerns time-averaging shifts. It's important to remember that measurement setups are sensitive to the surrounding environment, and are therefore subject to physical effects that can be subtle if not properly considered and controlled. A typical example is temperature variations in the surroundings of the measurement setup. If these are slow or of low intensity, and the acquisition campaign lasts a long time, a visual inspection of a small part of the measured traces may be insufficient to pinpoint the problem that span during the full acquisition campaign.\n", "\n", "Similarly to the other, use the following template in order to see the effect of the temporal shift. "]}, {"cell_type": "code", "execution_count": 27, "id": "f3a7f0be-8390-44bb-b20f-6f6c27b635e3", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_cpa, utils_files\n", "\n", "# Load the noisy version of the dataset\n", "dataset_noisy = utils_cpa.load_DCshift_dataset(utils_files.VALIDATION_DS[0], shift_mv=2.5)\n", "# Load the noise-free traces\n", "traces_origin = utils_files.load_dataset(utils_files.VALIDATION_DS[0])['traces']\n", "\n", "# Compute the correlation associated to both cases\n", "models = utils_cpa.model_HW_outSB(dataset_noisy['pts'].shape[1], traces_origin.shape[1])\n", "corr_noisy = utils_cpa.scalib_corr_traces(dataset_noisy['traces'], dataset_noisy['pts'],models)\n", "corr_origin= utils_cpa.scalib_corr_traces(traces_origin, dataset_noisy['pts'],models)"]}, {"cell_type": "code", "execution_count": 28, "id": "4fbdcf89-08a4-43bd-b06e-6f2a9c3c4f1b", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_plot\n", "%matplotlib widget\n", "import matplotlib.pyplot as plt \n", "\n", "# Byte index to plot\n", "# TODO: modify this value\n", "bindex = 8\n", "\n", "# Plot the figure\n", "utils_plot.display_two_correlations(corr_noisy, corr_origin, dataset_noisy['traces'], traces_origin, dataset_noisy[\"ks\"][0], bindex, \"With shift\")"]}, {"cell_type": "markdown", "id": "cc7689b5-b444-4ee0-a9b8-f23a4c8bda5b", "metadata": {}, "source": ["Adittionally, the following script allows you see the time average within the dataset, for successive chunks of fixed size (which should highlight the problem more). In addition to the shift, do you notice anything else special?\n", "\n", "*hint: the full dataset has been acquired in different chunks of 256 traces. Between these, the device had a small idle time where he was not perfomring any operation.*"]}, {"cell_type": "code", "execution_count": 29, "id": "eb8b3e20-22ce-4c05-8918-eb0bd90347de", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "def plot_temporal_average(traces_set, labels):\n", "    assert len(traces_set)==len(labels)\n", "    f = plt.figure()\n", "    ax = f.add_subplot(1,1,1)\n", "    for i, t in enumerate(traces_set):\n", "        temp_mean = np.mean(t,axis=1)\n", "        ax.plot(temp_mean, label=labels[i])\n", "        ax.set_ylabel(\"Temporal mean\")\n", "        ax.set_xlabel(\"Trace index\")\n", "    plt.legend()\n", "    plt.show()\n", "\n", "\n", "plot_temporal_average(\n", "    [traces_origin,dataset_noisy['traces']],\n", "    [\"Origin\", \"With shift\"])\n"]}, {"cell_type": "markdown", "id": "f11e500f-705b-4445-9ce7-01f009afd500", "metadata": {}, "source": ["### Going further: the HW model limitations"]}, {"cell_type": "markdown", "id": "b6a9f4d4-4571-47b2-9b78-03772e2e6604", "metadata": {}, "source": ["If you follow correctly, you remeber that the correlation metrics we've computed so far are based solely using leakage Hamming Weight model assumption. Although it seems somewhat relevant, it's worth asking whether the Hamming weight hyptohesis holds up in practice.\n", "\n", "In order to answer this question, the following code displays the scatter plot obtained with, on the one hand, the measurement point resulting in a maximum correlation (on the x-axis) and, on the other hand, the HW model of the latter (on the y-axis) for the 256 different possible classes with a fixed Sbox output byte. \n", "\n", "From what you observe, what can you conclude on the quality of the HW model? "]}, {"cell_type": "code", "execution_count": 30, "id": "adffba0c-a0f5-4a95-aaaf-069a5cd7efba", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_plot, utils_files\n", "\n", "# Load the dataset with all random inputs\n", "dataset = utils_files.load_dataset(utils_files.VALIDATION_DS[0])\n", "\n", "# TODO: change this parameter\n", "bindex = 8\n", "utils_cpa.scatter_HW_mean(dataset, bindex)"]}, {"cell_type": "markdown", "id": "55d0918e-c2a6-4d35-98c8-0a7f7f411953", "metadata": {}, "source": ["In the following sessions, we'll show you how to improve the latter through a profiling phase. This additional step \n", "enables the leakage model of the device to be estimated more accurately, and thus enables more efficient plates to be assembled (i.e., requiring less complex attacks) at the cost of a more or less complex training phase, depending on the models created."]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.3"}}, "nbformat": 4, "nbformat_minor": 5}