{"cells": [{"cell_type": "markdown", "id": "0849ee09-a6f1-4242-9f7c-280e9cc311d6", "metadata": {}, "source": ["# Modeling and extraction\n", "\n", "The previous section presented techniques for identifying POIs. In this section, we will focus on techniques for extracting information from these POIs in order to create a statistical model and use it to mount an attack. As a reminder, profiled attack are divided in two phases: the (offline) training phase, during which an adversary builds the model based on $q_{p}$ profiling traces, and the (online) attack phase during which he uses the model in order to recover sensitive information from $q_a$ (fresh) attack traces. Here, *Modeling* refers to the training phase while *extraction* refers to the attack phase in itself. \n", "\n", "In the profiled attack setting, we rely on two different datasets. The first is the training dataset contains measurements acquired using random plaintext and keys that are used to build the models while limiting bias. This one is typically acquired by the adversary during the offline phase by using the same device that he aims to attack. The second is the online dataset, which contains the measurements obtained when the targeted device performs executions using a fixed key value that it aims to recover (e.g., in the context of a real application). \n", "For our exercises, we use fixed-key datasets for which we know the value of the key. These datasets are called validation datasets, and are used to evaluate the practical performances of attacks carried out using the models you will build. \n", "\n", "In this session, we will look at statistical modelling strategy, ranging from textbook (univariate Gaussian Template) to more advanced techniques (Multivariate Gaussian Template with dimensionality reduction)."]}, {"cell_type": "markdown", "id": "85bab1e7-2237-4a21-a987-50126810186c", "metadata": {}, "source": ["## Gaussian Template Attack\n", "\n", "Next, we will carry out one of the most popular attacks: a Gaussian template attack. As detailed in the book in Section 2.3.1, this technique involves modeling the physical measurements as random variables that follow a Gaussian distribution, which may differ for the different classes. More into the details, the Probability Density Function (PDF) of the leakage trace $\\boldsymbol{l}$ measured conditioned to the fact that the intermediate value $z_i$ is manipulated can be expressed as:\n", "\n", "$$\\mathsf{f}(\\boldsymbol{l}|z_i) = \\frac{1}{(2\\pi)^{\\frac{\\tau}{2}}\\left|\\boldsymbol{\\Sigma}_{z_i}\\right|} \\cdot \\mathsf{exp}\\left( -\\frac{1}{2}\\left(\\boldsymbol{l} - \\boldsymbol{\\mu}_{z_i}\\right)^{\\intercal} \\cdot \\boldsymbol{\\Sigma}_{z_i}^{-1} \\cdot \\left( \\boldsymbol{l} - \\boldsymbol{\\mu}_{z_i}\\right)\\right)$$\n", "\n", "where $\\mu_{z_i}$ and $\\boldsymbol{\\Sigma}_{z_i}$ are respectively the means and the covariance matrix. The challenge of the profiling phase is therefore to estimate these (unknown) parameters of these distributions, which can be done by computing the empirical means and convariances:\n", "\n", "$$\n", "\\begin{eqnarray}\n", "    \\hat{\\boldsymbol{\\mu}}_{z_i} &=& \\hat{\\mathsf{E}}_{\\frac{q_p}{256}}\\left( \\boldsymbol{L}(z_i)\\right), \\\\\n", "    \\hat{\\boldsymbol{\\Sigma}}_{z_i} &=& \\hat{\\mathsf{E}}_{\\frac{q_p}{256}}\\left( \\left( \\boldsymbol{L}(z_i) - \\hat{\\boldsymbol{\\mu}}_{z_i}\\right) \\cdot \\left( \\boldsymbol{L}(z_i) - \\hat{\\boldsymbol{\\mu}}_{z_i}\\right)^{\\intercal} \\right).\n", "\\end{eqnarray}\n", "$$\n", "\n", "### Univariate Gaussian Template Attack\n", "To start simple, we will first focus on univariate gaussian templates. That is, we will only exploit a single POI when building the model associated to an intermediate variable. In the context of a univariate Gaussian distribution, the profilling phase boils down to estimating the distribution mean and variance, which can be approximated as:\n", "\n", "$$\n", "\\begin{eqnarray}\n", "\\mathsf{f}(\\boldsymbol{l}|z_i) \\approx \\hat{\\mathsf{f}}(\\boldsymbol{l}|z_i) &=& \\mathcal{N}\\left( \\boldsymbol{l}| \\hat{\\mu}_{z_i}, \\hat{\\sigma}^2_{z_i} \\right) \\\\\n", "&=& \\frac{1}{\\hat{\\sigma}_{z_i}\\sqrt{2\\pi}} \\hspace{2mm}\\exp\\left( - \\frac{\\left( l - \\hat{\\mu}_{z_i}\\right)^2}{2\\hat{\\sigma}_{z_i}^2}\\right)\n", "\\end{eqnarray}\n", "$$\n", "\n", "#### Step 1: POI selection\n", "\n", "As a first step, you have to identify the POI to use. While you could go back in the previous session in order to recover the one to by hand, we propose you to implement this selection automatically, by sorting the different time sample in a traces according to a decreasing order of SNR value. \n", "\n", "More practically, you have to implement the function `POI_selection_SNR`. We remind you that you've already seen how to compute the SNR using SCALib: for convinence, you can use the function `ref_snr_scalib` from `test_scale`. Its signature follows the same as the one you had to implement and its signature is written here as a reminder:\n", "\n", "```python\n", "def ref_snr_scalib(traces, classes, nclasses):    \n", "    \"\"\"\n", "    traces: the traces on which to compute the SNR, organised as a numpy array of float of shape (ntraces, nsamples)\n", "    classes: the variables classes for each trace, as an array of shape(ntraces, nvars).  \n", "    nclasses: the amount of different classes.\n", "    \n", "    return: the SNR values computed for each var at every time samples, as a numpy array of shape (nvars, nb_samples)\n", "    \"\"\"\n", "```\n", "More practically, you have to implement the function `POI_selection_SNR`:\n", "\n", "- *hint: First, compute the SNR, then sort the POI with [argsort](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html)*"]}, {"cell_type": "code", "execution_count": 1, "id": "7cbeb36c-1159-4f92-ad34-d14ac60c66f7", "metadata": {}, "outputs": [], "source": ["from utils_scale.test_scale import ref_snr_scalib\n", "import numpy as np\n", "\n", "#TODO implement this\n", "def POI_selection_SNR(traces, classes, nclasses):\n", "    \"\"\"\n", "    SNR based POIs selection\n", "    \n", "    traces: the training traces, as an array of shape (ntraces, nsamples).\n", "    classes: the label associated to each variables for every traces, as an array of shape  (ntraces, nvars)\n", "    nclasses: the amount of different classes. \n", "    \n", "    Returns: the ordered POIs for each variable, as an array of shape (nvars, nsamples). \n", "    More particularly, each row contains that index sorted in a decreasing SNR order (i.e., the first \n", "    element has the maximum SNR value). \n", "    \"\"\"\n", "    # TODO\n"]}, {"cell_type": "markdown", "id": "90ab9042-d37a-4d94-b53f-8b80fca82910", "metadata": {}, "source": ["You can test you function next"]}, {"cell_type": "code", "execution_count": 2, "id": "3c4961b7-8ac6-449a-90f2-3f332fd70af2", "metadata": {}, "outputs": [], "source": ["from utils_scale import test_scale\n", "test_scale.test_POI_selection_SNR(POI_selection_SNR)"]}, {"cell_type": "markdown", "id": "3a0a9b0f-9c84-4e1e-bfb5-fe11557b0e62", "metadata": {}, "source": ["In addition, the following script allows you to display the `npois` first POIs found when targeting the  `bindex`-th Sbox output ."]}, {"cell_type": "code", "execution_count": 3, "id": "cb3b9dbe-5b99-4f77-aa9e-0ffc0bc2bd95", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_files, utils_aes, test_scale\n", "%matplotlib widget\n", "import matplotlib.pyplot as plt\n", "\n", "# Load the dataset\n", "ds = utils_files.load_dataset(utils_files.TRAINING_DS[0])\n", "\n", "# TODO: change here\n", "npois = 1\n", "bindex = 0\n", "\n", "# Compute intermediate states \n", "classes = utils_aes.Sbox[ds[\"pts\"] ^ ds[\"ks\"]]\n", "# Compute the POIs based on your function\n", "pois = POI_selection_SNR(ds['traces'], classes, 256)\n", "# Display the selected POIS\n", "test_scale.display_pois_TA(pois[bindex, :npois], ds['traces'][0])\n"]}, {"cell_type": "markdown", "id": "8cfada57-2d7b-415f-889e-0ceced61295d", "metadata": {}, "source": ["#### Step 2: Model building (training phase)\n", "\n", "As a second step, you have to compute $\\hat{\\mu}_{z_i}$ and $\\hat{\\sigma}_{z_i}$. At first, taking into account the small training data complexity, we invite you to implement a pooled variance estimation: that is, we consider that the noise distribution is the same for every classes. In such a situation, a single noise variance is computed (per variable), and used in the subsequent models (as described in Section 2.3.1 of the book).\n", "\n", "More practically, you have to implement the function `univariate_gaussian_models`"]}, {"cell_type": "code", "execution_count": 4, "id": "24d2c2a8-fe02-4a67-a733-eb9053ef22fc", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "def univariate_gaussian_models(traces, classes, pois):\n", "    \"\"\"\n", "    traces: the training traces, as an array of shape (ntraces, nsamples).\n", "    classes: the label associated to each variables for every traces, as an array of shape  (ntraces, nvars)\n", "    pois: the single POI kept for each variable, as an array of shape (nvars,)\n", "    \n", "    return: (us, ss), where `us` and `ss` are of shape (nvars, 256) and contain respectively \n", "    the mean and the standard deviation associated every classes of the different variables.\n", "    \"\"\"\n", "    # TODO\n"]}, {"cell_type": "markdown", "id": "5ca4811a-bb3b-43c6-b8b7-5726129d63eb", "metadata": {}, "source": ["The following code snippet enables you to visualize the model you build in comparison to the empirical data used. In particular, you can choose which Sbox to target and the class to visualize with the parameters `bindex` and `vclass`. Check whether you model properly fit the data! "]}, {"cell_type": "code", "execution_count": 5, "id": "02a777d0-c31a-4472-afcb-360c4c701981", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_files, utils_aes, test_scale\n", "\n", "# Load the dataset\n", "ds = utils_files.load_dataset(utils_files.TRAINING_DS[0])\n", "\n", "# Compute intermediate states \n", "classes = utils_aes.Sbox[ds[\"pts\"] ^ ds[\"ks\"]]\n", "# Compute the POIs based on your function\n", "pois = POI_selection_SNR(ds['traces'], classes, 256)\n", "# Compute the models\n", "models_uni = univariate_gaussian_models(ds['traces'], classes, pois[:,0])"]}, {"cell_type": "code", "execution_count": 6, "id": "180fee8e-4307-447b-bf01-5b8489abc6c3", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_plot\n", "%matplotlib widget\n", "import matplotlib.pyplot as plt\n", "\n", "# Display visually the model\n", "# TODO: modify these\n", "bindex = 0\n", "vclass = 157\n", "\n", "utils_plot.plot_empirical_vs_model_dist(ds, classes, pois, models_uni, bindex, vclass)"]}, {"cell_type": "markdown", "id": "fd68becf-7def-494a-9629-f0aa745f665c", "metadata": {}, "source": ["Besides, you can use the following piece of code to display the model PDF for multiple classes. Try to look at the distributiosn for different traces!"]}, {"cell_type": "code", "execution_count": 7, "id": "9003b078-530a-4eff-8ada-526f8aedb212", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_plot\n", "%matplotlib widget\n", "import matplotlib.pyplot as plt\n", "\n", "# Display visually the model\n", "bindex = 0\n", "vclasses = [0,0b10101010, 0b01010101, 0xff]\n", "\n", "utils_plot.plot_empirical_vs_model_dist_several(ds, classes, pois, models_uni, bindex, vclasses)"]}, {"cell_type": "markdown", "id": "9a178d6a-764c-4ee1-a92d-c8d7aa9971b2", "metadata": {}, "source": ["#### Step 4: Exploiting the model (online attack phase)\n", "\n", "Once these models (also known as templates) are created, it remains to see how they can lead to practical attacks. During the extraction phase, an adversary typically has access to a target device with a fixed key that he seeks to retrieve. To this end, he obtains a set of fresh measurements for different executions and can perform a maximum likelihood attack based on these measurements and the models he has constructed. \n", "\n", "More precisely, for each trace, we can estimate the probability that yhr target variable belongs to a specific class. When targeting the output of the Sbox, the intermediate value $z_i$ used above can be expressed as $z_i=\\mathsf{Sbox}(p_i \\oplus k_i)$ and the aformentionned probability can be expressed as follows thanks to [Bayes' law](https://en.wikipedia.org/wiki/Bayes%27_theorem#For_continuous_random_variables)\n", "\n", "$$\n", "\\begin{eqnarray}\n", "\\check{k}_i &=& \\underset{k^*_i}{\\mathsf{argmax}}\\hspace{2mm}\\hat{\\mathsf{Pr}}\\left( k_{i}^* | p_i, \\boldsymbol{l}\\right) \\\\\n", "            &=& \\underset{k^*_i}{\\mathsf{argmax}}\\hspace{2mm}\\frac{\\hat{\\mathsf{f}}\\left( \\boldsymbol{l} | \\mathsf{Sbox}\\left(p_i \\oplus k_{i}^*\\right) \\right) \\cdot \\mathsf{Pr}(k^*_i)}{\\sum^{255}_{k'_i=0} \\hat{\\mathsf{f}}\\left( \\boldsymbol{l}|\\mathsf{Sbox}\\left(p_i \\oplus k'_{i}\\right)\\right) \\cdot \\mathsf{Pr}\\left( k'_i \\right)}\n", "\\end{eqnarray}\n", "$$\n", "\n", "where $\\mathsf{Pr}(x)$ is a shortcut for $\\mathsf{Pr}\\left(X =x \\right)$ for conciseness.\n", "\n", "Again, we will advance step by step here. To simplify your life, we've already provided you with the following function, which calculates the value of the Gaussian probability density function for fixed parameters. (Note, you can import this function from `utils_ta`)"]}, {"cell_type": "code", "execution_count": 8, "id": "9540aa36-ba01-41fc-acd5-1d6de9b4a4ce", "metadata": {}, "outputs": [], "source": ["def gaussian_pdf(xs, mean, std):\n", "    \"\"\"\n", "    Gaussian pdf\n", "\n", "    xs: the point for which we wish to compute, as an array of shape (nvar, )\n", "    mean: the mean of the distribution, as a float\n", "    std: the standard deviation of the distribution, as a float\n", "\n", "    \"\"\"\n", "    coef = 1/(std*np.sqrt((2*np.pi)))\n", "    return coef * np.exp(-(xs-mean)**2 / (2*(std**2)))"]}, {"cell_type": "markdown", "id": "1f096dab-ed39-4600-a2a0-0bb40702f459", "metadata": {}, "source": ["Next, you have to implement the first r `log2Pr_class` that compute the probabilities of every class of the targeted intermediate variable given the traces. That is, for each traces, you have to compute 256 different probabilities. This is done in practice by applying [Bayes' law](https://en.wikipedia.org/wiki/Bayes%27_theorem#For_continuous_random_variables) using the models built during the profiling phase."]}, {"cell_type": "code", "execution_count": 9, "id": "6bc182f8-beef-4577-ab67-7dd7e608fc8c", "metadata": {}, "outputs": [], "source": ["def log2Pr_class(traces, models):\n", "    \"\"\" \n", "    Compute the conditional probability of the classs given the leakages by applying Bayes law.\n", "\n", "    traces: the univariate leakage, as an array of shape (ntraces,nvars)\n", "    models: (us, ss) such as \n", "        us: the models means for each class, as an array of shape (nvars, nclasses)\n", "        ss: the models stds for each class, as an array of shape (nvars, nclasses)\n", "\n", "    Returns: an array of shape (nvars, ntraces, nclasses),\n", "    containing for every variables, for every traces, the log2 probability associated to each class, \n", "    \"\"\"\n", "\n"]}, {"cell_type": "markdown", "id": "2b4ef02e-d5b8-435e-9a6e-6509352b06f3", "metadata": {}, "source": ["Now, given the probabilities computed in the previous step, you have to implement the function `maximum_likelihood` that compute the probabilities associated to each subkey."]}, {"cell_type": "code", "execution_count": 10, "id": "8a488345-0860-4bdb-98ad-94fa3ade71c1", "metadata": {}, "outputs": [], "source": ["from utils_scale.utils_aes import Sbox\n", "def maximum_likelihood(pts, log2pr_sb):\n", "    \"\"\"\n", "    pts: plaintext, as an array of shape (ntraces, nvars)\n", "    log2pr: the log proba of the Sbox class, as an array of shape (nvars, ntraces, nclasses)\n", "\n", "    returns: the probas associated to each subkey bytes, as an array of shape (nvars, nclasses)\n", "    \"\"\"\n"]}, {"cell_type": "markdown", "id": "db3b0e53-cc78-4785-9b7f-98e1156a7e99", "metadata": {}, "source": ["Finally, you can implement the attack by mixing together the two previous functions!"]}, {"cell_type": "code", "execution_count": 11, "id": "fa1c26fb-cd98-42c3-a1c3-3639f3f5f7c1", "metadata": {}, "outputs": [], "source": ["def univariate_TA(traces, pts, pois, models):\n", "    \"\"\"\n", "    traces: the training traces, as an array of shape (ntraces, nsamples).\n", "    pts: the plaintext used, as an array of shape (ntraces, nvars)\n", "    pois: the list of pois to used, as an array of shape (nvars, )\n", "    models: (us, ss) such as \n", "        us: the models means for each class, as an array of shape (nvars, nclasses)\n", "        ss: the models stds for each class, as an array of shape (nvars, nclasses)\n", "\n", "    return: the probabilities associated to each subkey, as an array of shape (nvar, nclasses)\n", "    \"\"\"\n", "    "]}, {"cell_type": "markdown", "id": "67917a12-2960-4a4e-ae54-2be72fc2e8e4", "metadata": {}, "source": ["Try to run the attack with the following code snippet. How does the attack complexity compare with the one you obtained with the CPA? "]}, {"cell_type": "code", "execution_count": 12, "id": "cae331da-9f18-4d82-95a0-d5e6e98a4e36", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_files, utils_aes, test_scale\n", "\n", "# Load the dataset\n", "ds = utils_files.load_dataset(utils_files.VALIDATION_DS[0], seed_shuffle=0)\n", "\n", "# Amount of trace to used for the online attack phase\n", "q_a = 25\n", "\n", "# Performs the attack\n", "probas = univariate_TA(ds['traces'][:q_a], ds['pts'][:q_a], np.array([pois[:,0]]), models_uni )\n", "kguess = np.argmax(probas,axis=1)\n", "\n", "for kg, kc in zip(kguess, ds['ks'][0]):\n", "    print(\"Guessed {} (must be {}) --> {}\".format(kg, kc, \"SUCCESS\" if kg == kc else \"FAIL\"))"]}, {"cell_type": "markdown", "id": "c4a77ea0-fb99-47f6-aeef-713dbb2e06a6", "metadata": {}, "source": ["The following code snippet allows you to evaluate the attack's performance in a more general and detailed context. In particular, it performs attacks against the 5 validation datasets, with different attack complexities. Each subplot represents the evolution of the probability of each subkey as a function of attack complexity (with the correct value in black and the others in gray). What do you observe? Is the attack equally effective against all subkeys? "]}, {"cell_type": "code", "execution_count": 13, "id": "adc50f34-eca1-4a0c-a6fb-60a71dc87f7a", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_ta, utils_files\n", "\n", "# Dataset used for the training\n", "ds_train = utils_files.TRAINING_DS[0]\n", "\n", "# Datasets on which perform the attack for comparions\n", "ds_atcks = [utils_files.VALIDATION_DS[0]]\n", "\n", "# Profilling complexity\n", "qp = 16384\n", "\n", "# Attack complexities\n", "qas = range(35)\n", "\n", "v = utils_ta.explore_TA_univariate(ds_train, ds_atcks, qp, qas)\n", "utils_ta.display_explore_TA_univariate_result(v)\n"]}, {"cell_type": "markdown", "id": "bc0a3f0a-5982-4f84-a343-ec339c7be537", "metadata": {}, "source": ["## Improving the Template Attack with a more advanced training phase\n", "As you no doubt thought during the detection and mapping session, using only one point of interest can be sub-optimal given the number of points of appearing present in the traces. You were right!\n", "\n", "As described in Sections 2.3.1 and 2.3.2 of the book, a straightforward way to improve attacks to create multivariate models using multiple POIs in the traces and using dimensionality reduction techniques (e.g., using LDA or PCA). This section looks in more detail at this type of technique and enables you to manipulate them in order to observe the different trades-offs in terms of complexity (profiling and attack) that they involve.\n", "\n", "\n", "\n", "### First step: POI selection\n", "The first relevant question to ask is: which POIs should I choose? \n", "\n", "Let's consider SNR as a detection method. Naively, we would like to take the POIs as the time index sorted in descending order of SNR value. But is it a good strategy? Put in another way, are all the POIs equally relevant when considered simultaneously? We take a closer look at this question in the following exercise.\n", "\n", "The following code snippet is re-using what you already saw in order to display the SNR associated to a specific Sbox output. Besides, it lets you choose several POIs (by adding them to `pois`). This display allows you to check exactly which SNR value corresponds to the selected POI, as well as the visual position in the trace. Additionnally, a second cell allows to display the scatter plot between the leakage values corresponding to both POIs. \n", "\n", "We ask you to use this code in order to answer the following questions (we consider `bindex`=0 first):\n", "\n", "- Lets assume at first that you first POI is 3348. Considering the strategy maximizing the SNR described above. What would be the second POI? \n", "- Add it to the POI pool and plot the scatter plot. What do you observe? How do you explain it? \n", "- How many distinct SNR peaks (even small) do you observe?\n", "- Try to use POIs from different peaks and plot the scatter plots. What do you observe? What does it suggest?\n", "- Is it the same for other Sboxes?\n"]}, {"cell_type": "code", "execution_count": 14, "id": "9d01b49d-3073-49e3-9fcc-58c44e4128aa", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_files, utils_aes\n", "%matplotlib widget\n", "import matplotlib.pyplot as plt\n", "# Cropping window for display\n", "start, end =[2500,4000]\n", "\n", "# Selection of target\n", "# TODO: you can modify here\n", "bindex = 0\n", "\n", "# TODO modify here\n", "pois = [3606, 3740, 3301]\n", "\n", "\n", "# Load dataset, compute SNR for Sbox output\n", "ds = utils_files.load_dataset(utils_files.TRAINING_DS[0])\n", "classes = utils_aes.Sbox[ds[\"pts\"] ^ ds[\"ks\"]]\n", "snrs = ref_snr_scalib(ds['traces'], classes, 256)\n", "\n", "# Plot the result \n", "f= plt.figure()\n", "xs = range(start,end)\n", "ax0 = f.add_subplot(2,1,1)\n", "ax0.set_ylabel(\"Power\")\n", "ax1 = f.add_subplot(2,1,2)\n", "ax1.set_ylabel(r'SNR($SB_{{{}}})$'.format(bindex))\n", "ax0.plot(xs, ds['traces'][0,start:end])\n", "ax1.plot(xs, snrs[bindex,start:end])\n", "for poi in pois:\n", "    ax0.vlines(poi, ymin=min(ds['traces'][0,start:end]), ymax=max(ds['traces'][0,start:end]), linestyle=\"dashed\", color=\"red\")\n", "    ax1.vlines(poi, ymin=min(snrs[bindex,start:end]), ymax=max(snrs[bindex,start:end]), linestyle=\"dashed\",color=\"red\")\n", "f.tight_layout()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": 15, "id": "d75f49c6-ad73-4e84-9f1d-79d8a3c16bde", "metadata": {}, "outputs": [], "source": ["# Plot the scatter between the leakages values at both POIs\n", "f, axes = plt.subplots(len(pois),len(pois))\n", "f.set_figheight(12)\n", "f.set_figwidth(12)\n", "if len(pois)==1:\n", "    axes=np.array([[axes]])\n", "for i, p0 in enumerate(pois):\n", "    for j, p1 in enumerate(pois):\n", "        axes[i,j].scatter(ds['traces'][:,p0],ds['traces'][:,p1])\n", "        axes[i,j].axis('equal')\n", "        if j==0:\n", "            axes[i,j].set_ylabel(f'POI={p0}')\n", "        if i==0:\n", "            axes[i,j].set_title(f'POI={p1}')\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "b3300bec-426b-46af-bd09-986d28389327", "metadata": {}, "source": ["## Multivariate Gaussian Template with dimensionality reduction\n", "Next, you will deal with a more advanced model: the multivariate Gaussian Template. As already mentioned, we will use multiple POIs in order to extract more information. Before fitting the (multivariate) Gaussian parameters, we will apply a pre-processing step in order to reduce the dimension that will be considered in the distribution. As described more into the detail in Sections 2.3.1 and 2.3.2 of the book, this step avoids the need for profiling complexities that are difficult to achieve in practice (especially in the case considered in the training where the complexity is limited).\n", "\n", "More practically, we will start using Linear Discriminant Analysis (LDA). Instead of implementing all the computation by yourself, we invite you to rather rely on the SCALib library that already implements useful tools: the [LdaAcc](https://scalib.readthedocs.io/en/stable/source/api/scalib.modeling.LdaAcc.html#scalib-modeling-ldaacc) and the [Lda](https://scalib.readthedocs.io/en/stable/source/api/scalib.modeling.Lda.html#scalib-modeling-lda) objects. \n", "\n", "Your first task is to implement the function `multivariate_gaussian_models` that implements the profiling phase using SCALib tools.\n", "\n", "- *hint: the implementation of the function is **very** similar to LdaAcc example fomr SCALib's documentation*\n", "- *hint2: you may require to following function for formatting purpose: [ndarray.tolist](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.tolist.html#numpy-ndarray-tolist), [ndarray.astype](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.astype.html)*\n"]}, {"cell_type": "code", "execution_count": 16, "id": "dd661fe8-d002-4d5c-b424-915c9db74008", "metadata": {}, "outputs": [], "source": ["from scalib.modeling import Lda, LdaAcc\n", "\n", "#TODO: implement this function\n", "def multivariate_gaussian_models(traces, classes, pois, ndim):\n", "    \"\"\"\n", "    Multivariate Gaussian template with dimensionality reduction using LDA.\n", "    \n", "    We consider 256 classes for each variables.\n", "    \n", "    traces: the training traces, as an array of shape (ntraces, nsamples).\n", "    classes: the label associated to each variables for every traces, as an array of shape  (ntraces, nvars)\n", "    pois: the single POI kept for each variable, as an array of shape (nvars, npois)\n", "    ndim: number of dimensions to keep after dimensionality reduction for each variable, as an int.\n", "    \n", "    return: an solved 'Lda' instance for 'ndim' dimension.\n", "    \"\"\"\n"]}, {"cell_type": "markdown", "id": "cf21f853-d0c5-45e0-9924-96776934a53d", "metadata": {}, "source": ["Second, you have to adapt your implementation of `univariate_TA` in order to use your multivariate models. You have to implement the following function\n", "\n", "- *hint: The implementation is **very** similar to the funtion `univariate_TA`.Go get some inspiration from the solution notebook if you are lost!*\n", "- *hint2: Notice that the model are embedding the POIs. Thanks to SCALib API, these are not required anymore as an argument.*"]}, {"cell_type": "code", "execution_count": 17, "id": "1fccc18c-c17e-4a5d-bae3-22c2b01081d7", "metadata": {}, "outputs": [], "source": ["def multivariate_LDA_TA(traces, pts, models):\n", "    \"\"\"\n", "    traces: the training traces, as an array of shape (ntraces, nsamples).\n", "    pts: the plaintext used, as an array of shape (ntraces, nvars)\n", "    models: the 'Lda' instance build with 'multivariate_gaussian_models'\n", "\n", "    return: the probabilities associated to each subkey, as an array of shape (nvar, nclasses)\n", "    \"\"\"\n"]}, {"cell_type": "markdown", "id": "1702411e-d5db-484f-b56e-251dfa0fad11", "metadata": {}, "source": ["Let's try to analyze the attack complexity achieved with this new (more complex) model. For this purpose, here is a piece of code that runs attacks against the different validation dataset (similarly to what was done for the univariate TA). At this stage, we still select the POIs as the time samples sorted in a decreasing SNR value. Try to play a bit with the different parameters so see how they affect the attack capabilities:\n", "\n", "- What is the impact of increasing the amount of POIs? Can you increase it as much as you like without affecting the results of the attack?\n", "- Same question for the amount if dimnension in the linear subspace.\n", "- What is the best attack you can reach?"]}, {"cell_type": "code", "execution_count": 25, "id": "59256bd4-faa1-499b-96e2-3e9245d0ee91", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_ta, utils_files\n", "\n", "# Dataset used for the training\n", "ds_train = utils_files.TRAINING_DS[0]\n", "\n", "# Datasets on which perform the attack for comparions\n", "ds_atcks = [utils_files.VALIDATION_DS[0]]\n", "\n", "# Profilling complexity\n", "qp = 16384\n", "\n", "# Attack complexities\n", "qas = range(0,35)\n", "\n", "# TODO: modify here\n", "npois = 1\n", "ndim = 1\n", "\n", "\n", "\n", "\n", "v = utils_ta.explore_TA_multivariate(ds_train, ds_atcks, qp, qas, npois, ndim)\n", "utils_ta.display_explore_TA_univariate_result(v)\n"]}, {"cell_type": "markdown", "id": "0c8cba59-ab90-4501-9d59-d053b9b88bfd", "metadata": {}, "source": ["## A deeper investigation about the SNR based POI selection.\n", "For the last exercise, we suggest you take a closer look at POI selection. In particular, we propose to investigate whether a selection of POIs different from chosing the time index that have a decreasing SNR value can lead to better performance. To do this, the following code snippet lets you attack a subkey by selecting POIs by hand. The results obtained are compared with an attack using SNR-based selection, using the same number of POIs and the same number of projected dimensions.\n", "\n", "- *hint: try to use POI that are non redundant*\n", "- *hint2: The byte index 15 is of particular interest...*"]}, {"cell_type": "code", "execution_count": 19, "id": "80f8ea13-c8f2-4300-88f5-92e91366b844", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_files, utils_aes\n", "\n", "dstrain = utils_files.load_dataset(utils_files.TRAINING_DS[0])\n", "classes = utils_aes.Sbox[dstrain[\"pts\"] ^ dstrain[\"ks\"]]\n", "pois_snr = POI_selection_SNR(dstrain['traces'], classes, 256)\n", "\n", "# This can be usefull to have a look at how the SNR is selecting the POIs\n", "amount_poi_plotted=10\n", "for i in range(16):\n", "    print(f'SNR POIs class {i}: {pois_snr[i,:amount_poi_plotted]}')"]}, {"cell_type": "code", "execution_count": 30, "id": "dcd6dc33-4afe-4385-8b35-2d4c66637150", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "from utils_scale import utils_ta\n", "\n", "#### TODO edit here\n", "bindex = 15\n", "my_pois = [3717]\n", "ndim = 1\n", "\n", "\n", "# Validation dataset used\n", "ds_index = 0\n", "\n", "\n", "############# Dont modify from here ;)\n", "qas = range(20)\n", "model_snr = multivariate_gaussian_models(dstrain['traces'], classes[:,bindex][:,np.newaxis], pois_snr[bindex,:len(my_pois)][np.newaxis,:], ndim)\n", "model_mypois = multivariate_gaussian_models(dstrain['traces'], classes[:,bindex][:,np.newaxis], np.array([my_pois]), ndim)\n", "\n", "ds_atcks = [utils_files.VALIDATION_DS[ds_index]]\n", "prs_snr = np.zeros([len(ds_atcks),len(qas),256])\n", "prs_mypois = np.zeros([len(ds_atcks),len(qas),256])\n", "kc = np.zeros(len(ds_atcks),dtype=int)\n", "for dai,da in enumerate(ds_atcks):\n", "    dsa = utils_files.load_dataset(da, seed_shuffle=0)\n", "    kc[dai] = dsa[\"ks\"][0,bindex]\n", "    for qai, qa in enumerate(qas):\n", "        prs_snr[dai, qai] = multivariate_LDA_TA(dsa['traces'][:qa], dsa['pts'][:qa,bindex][:,np.newaxis], model_snr)\n", "        prs_mypois[dai, qai] = multivariate_LDA_TA(dsa['traces'][:qa], dsa['pts'][:qa,bindex][:,np.newaxis], model_mypois)\n", "\n", "\n", "f, axes = plt.subplots(2,1)\n", "axes[0].set_title(\"SNR based\")\n", "axes[1].set_title(\"Hand chosen\")\n", "for i in range(len(ds_atcks)):\n", "    print(f'Tipping points for DATASET {ds_index}')\n", "    print(\"-SNR based: {} traces.\".format(utils_ta.tipping_point(prs_snr[i], kc[i])))\n", "    print(\"-Hand chosen: {} traces.\".format(utils_ta.tipping_point(prs_mypois[i], kc[i])))\n", "    axes[0].plot(qas, prs_snr[i], color=\"xkcd:light grey\")\n", "    axes[1].plot(qas, prs_mypois[i], color=\"xkcd:light grey\")\n", "\n", "for i in range(len(ds_atcks)):\n", "    axes[0].plot(qas, prs_snr[i,:,kc[i]], color=\"black\")\n", "    axes[1].plot(qas, prs_mypois[i,:,kc[i]], color=\"black\")\n", "f.tight_layout()\n"]}, {"cell_type": "code", "execution_count": null, "id": "bef5fe9b-de34-4c04-ac63-6245156a616c", "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.3"}}, "nbformat": 4, "nbformat_minor": 5}