{"cells": [{"cell_type": "markdown", "id": "e568feb5-246b-4077-b6a3-2c704a65ce04", "metadata": {}, "source": ["## Hands-on: Profiled Correlation Power Analysis\n", "\n", "The last session concluded with the observation that the Hamming Weight leakage hypothesis has certain limitations: although it allows significant correlation values to be obtained, the last exercise clearly showed that classes assumed to have the same Hamming Weight had very different mean trace behavior"]}, {"cell_type": "markdown", "id": "589626f3-69bc-451c-8851-24c8af6dc13a", "metadata": {}, "source": ["As an introduction to profiled attacks, we propose to overcome these limitations and implement a so-called profiled correlation power analysis. The idea behind it is quite simple: instead of using an a priori known model to model leakage, we'll use a profile of it. \n", "\n", "Remember the first session, in which we approximated the behavior of the traces associated with the fixed value of the target variable as the average of different traces measured when this value was manipulated. The idea here is the same: instead of using an a priori-known model, we're going to create it ourselves on the basis of actual measurements during a so-called training phase. To do this, we'll operate in two steps: (1) we'll create the models in a first step, and (2) we'll perform a classic CPA, but using our brand new models instead of HW values."]}, {"cell_type": "markdown", "id": "23886860-67af-4b13-9164-aecd54719c64", "metadata": {}, "source": ["### Step 1: building the models\n", "In this section, we'll create the models together. As already mentioned, we'll calculate the average trace associated with each Sboxes output, for all 256 possible values. The aim of this step is to calculate the $16\\times256=4096$ different averaged traces.\n", "\n", "To help you solve the exercise, we'll divide the construction into different sub-steps. The first consists in calculating the average trace associated with a given class for a single variable. To do so, you have to implement the function `empirical_model_single_class` defined next:\n"]}, {"cell_type": "code", "execution_count": 1, "id": "e0b3a53d-009a-49ef-a235-c51c5c67dffa", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "# TODO: implement this function\n", "def empirical_model_single_class(traces, var_labels, label):\n", "    \"\"\"\n", "    Compute the empirical model associated to a single class of a single variable.\n", "\n", "    traces: the traces, as an array of shape (ntraces, nsamples)\n", "    var_labels: the class labels taken by the variable for each traces, as an array of shape (ntraces,)\n", "    label: the targeted class\n", "\n", "    return: the averaged trace corresponding to the variable class, as an array of shape (nsamples,)\n", "    \"\"\"\n"]}, {"cell_type": "markdown", "id": "154d16c8-52ba-4c67-b5b6-7c0c602d0a4b", "metadata": {}, "source": ["You can verify your implementation here:"]}, {"cell_type": "code", "execution_count": 2, "id": "dcf2a515-36c8-4e0d-8853-e0573d11c1b9", "metadata": {}, "outputs": [], "source": ["from utils_scale import test_scale\n", "test_scale.test_empirical_model_single_class(empirical_model_single_class)"]}, {"cell_type": "markdown", "id": "44f46664-f168-4812-b9b2-e640f823dc5b", "metadata": {}, "source": ["In a second phase, your aim is to integrate this function in order to calculate all the models exhaustively. To do so, you have to implement the function `cpa_empirical_model_sbox_out` defined next:\n"]}, {"cell_type": "code", "execution_count": 3, "id": "592e29d1-81ff-4c4a-8b84-6040cb3761db", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "# TODO: implement this function\n", "def empirical_model(traces, labels, nclasses):\n", "    \"\"\"\n", "    Compute the empirical model associated to the Sbox outputs provided.\n", "    \n", "    traces: the training traces, as numpy array of shape (ntraces, nsamples)\n", "    labels: the class label of the variables (e.g., sboxes outputs), as a numpy array of shape (ntraces, nvars). \n", "            the labels MUST be in range [0, nclasses-1]\n", "    nclasses: the amounf of different classes considered (e.g., 256 if we target 8-bit variables).\n", "    \n", "    returns: the models, as a numpy array of shape (nvars, classes, nsamples), where\n", "    models[i,j,k] is the model of the k-th samples in the traces, for the j-th class of the i-th variable. \n", "    \"\"\"\n"]}, {"cell_type": "markdown", "id": "53928910-a69f-439b-b273-c8e0ec045bac", "metadata": {}, "source": ["You can verify your implementation here"]}, {"cell_type": "code", "execution_count": 4, "id": "005fc276-d505-47c1-90cb-525a65898a1a", "metadata": {}, "outputs": [], "source": ["from utils_scale import test_scale\n", "test_scale.test_cpa_empirical_model_sbox_out(empirical_model)"]}, {"cell_type": "markdown", "id": "fbe24b0d-3f44-4970-9f08-10cef2cb0683", "metadata": {}, "source": ["## Step 2: using the model\n", "\n", "Now that we've got what we need to create the models, all that's left is to exploit them. Before implementing the attack, we'll first check whether they can improve the correlation values obtained. The following code snippets compute and display the correlation for a single byte, both using the HW and you empirical model. \n", "\n", "What do you observe? What do you expect regarding the performances of the CPA? \n"]}, {"cell_type": "code", "execution_count": 5, "id": "ada8c4da-16dc-4749-8c97-167185b1e509", "metadata": {}, "outputs": [], "source": ["from utils_scale.test_scale import ref_pearson_corr\n", "from utils_scale import utils_files, utils_aes, utils_cpa, utils_plot\n", "%matplotlib widget\n", "import matplotlib.pyplot as plt\n", "\n", "# Cropping windows\n", "# TODO, you may want to change here\n", "start = 2500\n", "end = 4500\n", "\n", "# Load the dataset\n", "ds = utils_files.load_dataset(utils_files.TRAINING_DS[0],cropping=[start,end])\n", "\n", "# build the Hamming Weight model\n", "hw_model = np.tile(utils_aes.HW[np.newaxis,:,np.newaxis], (16, 1, ds['traces'].shape[1])).astype(np.float64)\n", "# Compute the Empirical model\n", "classes = utils_aes.Sbox[ds['pts'] ^ ds['ks']]\n", "emp_model = empirical_model(ds['traces'], classes, 256)\n", "\n", "bindex = 2\n", "# Compute the correlation\n", "corr_hw = ref_pearson_corr(ds[\"traces\"], hw_model[bindex,classes[:,bindex]])\n", "corr_emp = ref_pearson_corr(ds[\"traces\"], emp_model[bindex,classes[:,bindex]])\n", "\n", "# Display\n", "utils_plot.display_HW_emp_corr(ds['traces'][0], corr_hw, corr_emp)\n", "    "]}, {"cell_type": "markdown", "id": "1914d48e-9cbc-4cba-bbfc-0d73f571c0af", "metadata": {}, "source": ["As a final step, try to implement the full CPA using the model you build. "]}, {"cell_type": "code", "execution_count": 6, "id": "a8d5c7c8-f65f-475d-a8de-c29b97ef2c0c", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from utils_scale import utils_files, utils_cpa\n", "from utils_scale import utils_aes\n", "%matplotlib widget\n", "import matplotlib.pyplot as plt\n", "#TODO implement the following function\n", "def cpa_out_sbox_profiled(traces, pts, models_sbox_out):\n", "    \"\"\" \n", "    Perform a CPA targeting the sbox output, for several subkey, using a defined model.\n", "\n", "    traces: the traces, as an array of shape (nexec, nsamples)\n", "    pts: the plaintexts used, as an array of shape (nexec, nvars)\n", "    models: the models to use, as an array of shape (nvars, nclasses, nsamples)\n", "\n", "    Returns: a NumPy array of shape (nvars,), with the best key candidate \n", "    \"\"\"\n"]}, {"cell_type": "markdown", "id": "0d9b9e1f-93ce-4b88-bbba-05d959aacc73", "metadata": {}, "source": ["### Mixing step 1 and 2"]}, {"cell_type": "markdown", "id": "b635ae09-fbf0-4d35-9e7f-bf5d77e84904", "metadata": {}, "source": ["Try to adapt the following template in order to implement the two steps of the CPA. How does the attack complexity compare with the HW CPA? How do you explain it?"]}, {"cell_type": "code", "execution_count": 7, "id": "23589cf1-1f48-4fc6-97f6-dea8da64b277", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_files, utils_cpa\n", "### STEP 1: model building\n", "# Dataset used to build the profiles. We use as much traces as possible here\n", "ds = utils_files.load_dataset(utils_files.TRAINING_DS[0])\n", "\n", "# Create the models\n", "# TODO: adapt here in order to assign the variable emp_model.\n", "# it should contain the empirical model as an array of shape (16,256,nsamples)\n", "emp_model=None\n"]}, {"cell_type": "code", "execution_count": 24, "id": "4a75997c-3845-40c3-aa61-ad6fcde00a0c", "metadata": {}, "outputs": [], "source": ["### Step 2: attack\n", "# Dataset using a fixed key that we attack\n", "ds = utils_files.load_dataset(utils_files.VALIDATION_DS[2])\n", "\n", "# TODO: modify here\n", "q_a = 57\n", "\n", "# CPA \n", "# TODO: try to adapt here. The value of kguess MUST be an array of 16 bytes\n", "# corresponding to the key you guessed\n", "kguess = None\n", "\n", "for kg, kc in zip(kguess, ds[\"ks\"][0]):\n", "    print(f\"found {kg} --> must be {kc}: {\"SUCCESS\" if kg==kc else \"FAILURE\"}\")"]}, {"cell_type": "markdown", "id": "4c4f7c54-623d-4b69-b00b-c313e433969e", "metadata": {}, "source": ["# Detection and mapping\n", "\n", "In general, profiled attacks are divided into two distinct phases: the profiling phase and the attack phase. In the first phase, an adversary is assumed to have access to a device similar to the one being attacked to create a leakage model. This model is then used in the attack phase to retrieve information about the value of the key manipulated by the targeted device.\n", "\n", "Although the profiling phase makes it possible to reduce the number of traces needed to carry out the attack during the online phase, it comes with the difficulty of having to profile the device, which can be costly depending on the length of traces and the number of dimensions used. Faced with these limitations, detection and mapping methodologies emerge as a first strategy to reduce the profiling overheads. Their goal is to identify the time samples in the traces that carry information, allowing an adversary to focus data manipulation efforts only on these. Such time samples are depicted as Points Of Interest (POIs).\n", "\n", "Next, we will deal with detection strategies commonly encountered in the literature. These have different advantages and disadvantages, and their use mainly depends on the opponent's objectives and constraints.\n", "\n", "To identify POIs, we need to define what we are looking for when examining the traces. A hypothesis commonly used in the literature is to consider that leakage with respect to an intermediate variable can be characterized in two parts: the first is deterministic and depends on the target value, and the second is random and represents noise. Using the notation from Section 2.2 of the book, the $j$-th time sample of a trace with respect to the S-box output value denoted $z_i$ can be represented as:\n", "\n", "\n", "$$l_j(z_i) = \\delta_j (z_i) + r_j$$\n", "\n", "where $\\delta_j$ is the leakage deterministic part of the $j-$th time sample and $r_j$ is an additive noise.\n", "\n", "In fact, you've already used a detection strategy: correlation! Remember the exercises related to the (Profiled-)CPA, the correlation peaks displayed typically appear at the time sample where the intermediate state used to model the leakage is actually manipulated by the implementation!\n", "Next, we will see two others commonly used POIs detection techniques: the Signal-to-Noise Ratio and the Welch's T-test.\n"]}, {"cell_type": "markdown", "id": "9c6b8990-4130-40de-8c9c-5b74391d247f", "metadata": {}, "source": ["## Signal-to-Noise Ratio (SNR)\n", "\n", "The second detection method we cover here is the Signal-to-Noise Ratio (SNR). As hinted by its name, it consists of computing the ratio between the signal and the noise levels, as detailed in Section 2.2.2 of the book.\n", "\n", "On one hand, the signal can be intuitively seen as the variation between the deterministic part of the signal for each possible value taken by \\( z_i \\). However, characterizing this precisely in practice can be challenging. Instead, it is estimated by averaging several traces belonging to the same class (i.e., the traces collected for a given value taken by \\( z_i \\)), which is\n", "\n", "$$\\hat{\\delta}_j(z_i) = \\hat{\\mathsf{E}}(L_j(z_i))$$. \n", "\n", "On the other hand, the noise is the variation that occurs in the measurements when the deterministic part is fixed. It can be estimated as the variance of the noise samples, defined as\n", "\n", "$$r_j = l_j(z_i) - \\hat{\\delta}_j(z_i)$$\n", "\n", "In our AES case study, we are interested in detecting the leakage of the independent 16 bytes at the S-box outputs. When targeting a single byte, the dataset composed of \\( q \\) traces is split into 256 different subsets (i.e., 1 per class) in order to estimate the means and the variances. Usually, the SNR is computed using intermediate variables following a uniform distribution in order to avoid biases, and the resulting subsets are composed of \\( q/256 \\) traces each. In such a case, the SNR is defined as (see Section 2.2.2, Eq. (5) in the book):\n", "\n", "$$\\hat{\\mathsf{SNR}}_q = \\dfrac{\\hat{\\mathsf{Var}}_{256}(\\hat{\\mathsf{E}}_{q/256}(L_j(z_i)))}{\\hat{\\mathsf{E}}_{256}(\\hat{\\mathsf{Var}}_{q/256}(L_j(z_i)))}$$ \n", "\n", "Next, your goal is to implement the SNR. For this exercise, we consider only the SNR related to a single 8-bit intermediate variable, so you can consider that the class indexes provided span only in the range [0, 255]. \n", "To help you get some intuition on what it is, we split the implementation such that it can be carried out step by step. \n", "\n", "First, you have to compute the so-called \"signal\" associated to a specific 8-bit variable by implementing the function `compute_byte_signal`, defined next:\n", "\n", "-*hint: First, compute the means of the subset of traces corresponding to a given class. Then, compute the variance of the means you just computed.*\n"]}, {"cell_type": "code", "execution_count": 9, "id": "e3478b14-53f9-4de1-b90b-bbb791c67cfd", "metadata": {}, "outputs": [], "source": ["def compute_byte_signal(traces, classes):\n", "    \"\"\"\n", "    Compute the signal part of the SNR (i.e., its numerator)\n", "    \n", "    traces: the traces on which to compute the SNR, organised as a numpy array of float of shape (ntraces, nsamples)\n", "    classes: for each traces, the class index associated as a numpy array of shape (ntraces, ). \n", "    \n", "    return: the signal (i.e., numerator) part of SNR the computed for each time samples, as an array of shape (nsamples,)\n", "    \"\"\"\n"]}, {"cell_type": "markdown", "id": "a4456342-ade3-4cab-917e-b8869df04b90", "metadata": {}, "source": ["You can verify your implementation next:"]}, {"cell_type": "code", "execution_count": 10, "id": "a8fbeea1-fa91-406c-8044-1711b59bfef4", "metadata": {}, "outputs": [], "source": ["from utils_scale import test_scale\n", "test_scale.test_snr_signal(compute_byte_signal)"]}, {"cell_type": "markdown", "id": "a9731052-be9f-4b37-b946-341ab1687bf8", "metadata": {}, "source": ["Second, you have to compute the so-called \"noise\" associated to a specific 8-bit variable by implementing the function `compute_byte_noise`, defined next:\n", "\n", "-*hint: First, compute the variance of the subset of traces corresponding to a given class. Then, compute the mean of the variances you just computed.*"]}, {"cell_type": "code", "execution_count": 11, "id": "da9b2600-9bfc-4f71-b2b9-9b215adeab23", "metadata": {}, "outputs": [], "source": ["def compute_byte_noise(traces, classes):\n", "    \"\"\"\n", "    Compute the signal part\n", "    \n", "    traces: the traces on which to compute the SNR, organised as a numpy array of float of shape (ntraces, nsamples)\n", "    classes: for each traces, the class index associated as a numpy array of shape (ntraces, ). \n", "    \n", "    return: the signal (i.e., numerator) part of SNR the computed for each time samples, as an array of shape (nsamples,)\n", "    \"\"\"\n"]}, {"cell_type": "markdown", "id": "ca00e95f-0552-4ba7-bdd5-15f83e869f1e", "metadata": {}, "source": ["You can verify your implementation next:"]}, {"cell_type": "code", "execution_count": 12, "id": "9f7fcffb-d666-4669-a18b-ce533c728b46", "metadata": {}, "outputs": [], "source": ["from utils_scale import test_scale\n", "test_scale.test_snr_noise(compute_byte_noise)"]}, {"cell_type": "markdown", "id": "9bf360b1-e8cd-47c0-9c7e-de1250351066", "metadata": {}, "source": ["Finally, you can implement the SNR in itself! You have to implement the function `compute_byte_snr` defined next:\n", "\n", "- *NB: Relying directly on `compute_byte_signal` and `compute_byte_noise` you implemented before is sub-optimal. Don't hesitate to write it from scratch if you feel so!*"]}, {"cell_type": "code", "execution_count": 13, "id": "6dfe6b31-dc87-4cd4-a5ec-5970e1e75bbb", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "def compute_byte_snr(traces, classes):\n", "    \"\"\"\n", "    traces: the traces on which to compute the SNR, organised as a numpy array of float of shape (ntraces, nsamples)\n", "    classes: for each traces, the class index associated as a numpy array of shape (ntraces, ). \n", "    \n", "    return: the SNR values computed for each time samples, as an array of shape (nsamples,)\n", "    \"\"\"\n", "    # TODO\n", "\n"]}, {"cell_type": "markdown", "id": "13fcc053-561d-4f92-954f-e95c17254b77", "metadata": {}, "source": ["You can verify your implementation here"]}, {"cell_type": "code", "execution_count": 14, "id": "e0489e26-93c5-4093-932b-bbe8afe0555c", "metadata": {}, "outputs": [], "source": ["from utils_scale import test_scale\n", "test_scale.test_snr_byte(compute_byte_snr)"]}, {"cell_type": "markdown", "id": "01ffb6da-8ce0-426a-a6e6-a4e6e10a6b90", "metadata": {}, "source": ["Use the following piece of code to display the computed SNR for the different bytes at the output of the S-box layer. What do you observe? How do the results compare with the profiled correlation POI detection strategy? How do you explain it?"]}, {"cell_type": "code", "execution_count": 15, "id": "133bfd77-739a-4f5e-8efc-dd408a7dad7c", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_files, test_scale\n", "from utils_scale.utils_aes import Sbox\n", "\n", "start = 2500\n", "end = 4500\n", "ds = utils_files.load_dataset(utils_files.TRAINING_DS[0],cropping=[start,end])\n", "\n", "#TODO: modify here \n", "bindex= 2\n", "\n", "labels = Sbox[ds['pts'] ^ ds['ks']]\n", "test_scale.display_snr_sbox_output(\n", "    ds['traces'], labels, bindex, compute_byte_snr,\n", "    subplots=[\n", "        [compute_byte_signal, \"Signal\"],\n", "        [compute_byte_noise, \"Noise\"],\n", "    ])"]}, {"cell_type": "markdown", "id": "d21e6551-b553-4fed-8a80-6f3381e227bb", "metadata": {}, "source": ["Remember SCALib? The library also contains an implementation of the [SNR](https://scalib.readthedocs.io/en/stable/source/api/scalib.metrics.SNR.html#scalib.metrics.SNR). Try computing the SNR using SCALib and implement the function `compute_snr_scalib`."]}, {"cell_type": "code", "execution_count": 16, "id": "e0bf7459-278c-4f73-8bb0-24ce4bff68b1", "metadata": {}, "outputs": [], "source": ["from scalib.metrics import SNR\n", "\n", "def compute_snr_scalib(traces, classes, nclasses):    \n", "    \"\"\"\n", "    traces: the traces on which to compute the SNR, organised as a numpy array of float of shape (ntraces, nsamples)\n", "    classes: the variables classes for each trace, as an array of shape(ntraces, nvars).  \n", "    nclasses: the amount of different classes.\n", "    \n", "    return: the SNR values computed for each var at every time samples, as a numpy array of shape (nvars, nb_samples)\n", "    \"\"\"\n", "    # TODO\n"]}, {"cell_type": "markdown", "id": "f7e10881-963e-4a98-80d5-1bff6f830c79", "metadata": {}, "source": ["You can test your implementation here"]}, {"cell_type": "code", "execution_count": 17, "id": "b103f919-c165-4d99-8f7f-31013d36f6df", "metadata": {}, "outputs": [], "source": ["from utils_scale import test_scale\n", "test_scale.test_snr_scalib(compute_snr_scalib)"]}, {"cell_type": "markdown", "id": "d5c4b7d7-efa2-44e7-8b8b-6c0feca553bd", "metadata": {}, "source": ["## Welch's T-test\n", "\n", "The next detection method discussed is the Welch T-test, as developed in Section 2.2.3 of the book. It allows reducing the data complexity required for POI detection by decreasing the number of classes for which the signal must be estimated.\n", "\n", "As a reminder, the method consists of conducting a Welch T-test statistical test to observe if a difference of means exists between two sets of measurements (similarly to what you performed visually in the first session, but in a more systematic way). Here, the first setting we consider relies on the following sets:\n", "\n", "\n", "- a *fixed* set of $q_{\\text{f}}$ measurements, using a fixed plaintext $x_{\\text{f}}$ and a fixed key $k_{\\text{f}}$ for each trace.\n", "- a *random* set of $q_{\\text{r}}$ measurements, using random plaintext drawn uniformly and the fixed key $k_{\\text{f}}$ for each trace.\n", "\n", "Generally, a global dataset of $q$ measurements is collected such that $q_{\\text{f}} \\approx q_{\\text{r}} \\approx q/2$ and the latter is split in order to obtain the two datasets. From these, we can compute the following estimates for the $j$-th time sample:\n", "\n", "$$\n", "\\begin{eqnarray}\n", "\\hat{\\mu}_{\\text{f}}(j) &=& \\hat{\\mathsf{E}}_{q_{\\text{f}}}\\left( L_j(x_{\\text{f}}, k_{\\text{f}})\\right), \\\\\n", "\\hat{\\mu}_{\\text{r}}(j) &=& \\hat{\\mathsf{E}}_{q_{\\text{r}}}\\left( L_j(\\$, k_{\\text{f}}))\\right), \\\\\n", "\\hat{\\sigma}_{\\text{f}}^2(j) &=& \\hat{\\mathsf{Var}}_{q_{\\text{f}}}\\left( L_j(x_{\\text{f}}, k_{\\text{f}})\\right), \\\\\n", "\\hat{\\sigma}_{\\text{r}}^2(j) &=& \\hat{\\mathsf{Var}}_{q_{\\text{r}}}\\left( L_j(\\$, k_{\\text{f}}))\\right),\n", "\\end{eqnarray}\n", "$$\n", "\n", "where $\\$$ denotes a uniformly random value. Based on these, the T statistic for all time samples can be computed as:\n", "\n", "$$\\hat{\\Delta}_{\\frac{q}{2}}(j) = \\dfrac{\\hat{\\mu}_{\\text{f}}(j) - \\hat{\\mu}_{\\text{r}}(j)}{\\sqrt{\\frac{\\hat{\\sigma}_{\\text{f}}^2(j)}{q_{\\text{f}}}} + \\sqrt{\\frac{\\hat{\\sigma}_{\\text{r}}^2(j)}{q_{\\text{r}}}}}$$\n", "\n", "Before seeing what Ttest can achieve, try to understand how the first order (i.e., d=1) Ttest is implemented using [SCALib](https://scalib.readthedocs.io/en/stable/source/api/scalib.metrics.Ttest.html#scalib.metrics.Ttest), as presented next:\n", "\n", "\n", "\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": 18, "id": "95bc3249-f2fc-490a-b829-4a617e10222f", "metadata": {}, "outputs": [], "source": ["from scalib.metrics import Ttest\n", "\n", "def ttest_computation(traces, labels):\n", "    \"\"\"\n", "    traces: matrix of shape (ntraces, nsamples) containing the traces of the first set used in the statistical test\n", "    labels: vector of shape (ntraces), containing the label of the class for each traces (i.e., either 0 or 1).\n", "    return: a vector of shape (nb_samples,) containing the statistic values for each time samples. \n", "    \"\"\"\n", "    ttest_obj = Ttest(d=1)\n", "    ttest_obj.fit_u(np.round(traces).astype(np.int16), labels.astype(np.uint16))\n", "    return ttest_obj.get_ttest()[0]\n"]}, {"cell_type": "markdown", "id": "f001baa4-62db-4415-8d36-e030fd83f77b", "metadata": {}, "source": ["A popular rule-of-thumb assumes POIs to be detected if the t-statistic is higher than a threshold value of 4.5, which corresponds to a probability to reject the null hypothesis below $ 10^{-5} $ (the null hypothesis being that there is no difference of mean between the fixed and random leakage sets).\n", "\n", "Next, we want you to identify interesting time samples using the T-test as a detection method in the setting described above (we plot the threshold of 4.5 with a solid horizontal red line). \n", "\n", "For the first test, we propose to reproduce the analysis you performed visually, but this time using the t-statistic. We first get back to the comparison between the following datasets:\n", "- `dataset1` that uses ($p_0$;$k_0$)\n", "- `dataset3` that uses ($p_0$;$k_1$), and therefore uses the same plaintext as `dataset1`, but uses a different key.\n", "\n", "Try tu run the following piece of code:\n", "\n", "- What do you observe?\n", "- Is this consistent with the visual observations you made earlier?"]}, {"cell_type": "code", "execution_count": 19, "id": "f13c3b88-07a5-41bc-a9f9-e6b2358cbd32", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from utils_scale import utils_obs, test_scale\n", "%matplotlib widget\n", "import matplotlib.pyplot as plt\n", "\n", "## Load the dataset with different keys\n", "ds1 = utils_obs.load_dataset_p0k0()\n", "ds2 = utils_obs.load_dataset_p0k1()\n", "\n", "trs = np.vstack([\n", "    ds1['traces'],ds2['traces']\n", "])\n", "\n", "labels = np.array(ds1['traces'].shape[0]*[0] + ds1['traces'].shape[0]*[1])\n", "\n", "tstat = ttest_computation(trs, labels)\n", "test_scale.display_ttest_result(tstat, ds1['traces'][0], title=\"pt fixed ; k different\")"]}, {"cell_type": "markdown", "id": "87af8589-c219-4d4f-bb00-dcde2945eb3b", "metadata": {}, "source": ["For the second test, we use the following dataset instead: \n", "\n", "- `dataset1` that uses ($p_0$;$k_0$),\n", "- `dataset2` that uses ($p_1$;$k_0$), and therefore uses the same key as `dataset1`, but uses a different plaintext.\n", "\n", "Try answer the following question:\n", "\n", "- How does it compare to the previous test?\n", "- Is it expected?\n", "- According to the 4.5 threshold criterion, what can you conclude? Does it make sense? Why is it happening?\n", "\n", "Note that the following template can help you.\n", "\n", "- *hint: As a reminder, the region in the traces between ~1600 and ~3000 correspond the the key scheduling operations that only depend on the key value*\n", "- *hint2: The two datasets compared have been acquired sequentially, at (close yet) different moment in time.*"]}, {"cell_type": "code", "execution_count": 20, "id": "b2210c76-36a3-443b-bc46-6aa7281ce6af", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from utils_scale import utils_obs, test_scale\n", "%matplotlib widget\n", "import matplotlib.pyplot as plt\n", "\n", "## Load the dataset with different keys\n", "ds1 = utils_obs.load_dataset_p0k0()\n", "ds2 = utils_obs.load_dataset_p1k0()\n", "\n", "trs = np.vstack([\n", "    ds1['traces'],ds2['traces']\n", "])\n", "\n", "labels = np.array(ds1['traces'].shape[0]*[0] + ds1['traces'].shape[0]*[1])\n", "\n", "tstat = ttest_computation(trs, labels)\n", "test_scale.display_ttest_result(tstat, ds1['traces'][0], title=\"k fixed ; pt different\")"]}, {"cell_type": "markdown", "id": "cf4b40d4-ce60-4da1-b5af-89d2313e848b", "metadata": {}, "source": ["The observations you have made above are a good example of how the very high sensitivity of the Ttest is both a strength and a flaw. In particular, if we refer strictly to the 4.5 threshold, the Test above seems to indicate that the keys are different (which is not the case). Because of its ultra-sensitivity, using the Ttest requires the creation of suitable setups to ensure that there is no other dependency in the consumption measurement than the data-dependent one, which as you have seen in the previous sessions is conducive to many technicalities.\n", "\n", "In our case, we can observe that the temporal mean of the two datasets tend to be different for some part of the dataset (e.g., for the ~600 first traces as well as between the 1200-th and the 2000-th traces). This is potentially due to the fact that they were collected independently and may have undergone different environmental influences. This (small) difference may be sufficient to obtain a t-value exceeding the threshold level. \n", "\n", "You can run the following script to display the temporal mean of every traces in both datasets, as well as their global means. "]}, {"cell_type": "code", "execution_count": 21, "id": "17457bd9-e225-47aa-bb20-186747ba80b1", "metadata": {}, "outputs": [], "source": ["from utils_scale import utils_plot\n", "\n", "## Load the dataset with different keys\n", "ds1 = utils_obs.load_dataset_p0k0()\n", "ds2 = utils_obs.load_dataset_p1k0()\n", "\n", "utils_plot.plot_temporal_average(\n", "    [ds1['traces'],ds2['traces']],\n", "    [\"p0\", \"p1\"])"]}, {"cell_type": "markdown", "id": "daa1c70d-753e-4800-b99a-e50698e95edb", "metadata": {}, "source": ["One way of reducing the apparition of these biases is to interleave the acquisition of several datasets in parallel. In practice, the configuration used for each measured execution (e.g., which plaintext is used in our example) is drawn randomly. The following code re-runs the same experiments with a dataset that has been acquired using such interleaving.\n", "\n", "What do you observe?\n", "\n", "- *NB: you can notice around the 500-th trace that a temporal mean shift likely occured which impacted similary both dataset due to interleaving!*"]}, {"cell_type": "code", "execution_count": 22, "id": "edb6982b-9511-4d9b-a468-6fa462fc9e42", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from utils_scale import test_scale, utils_files, utils_plot\n", "%matplotlib widget\n", "import matplotlib.pyplot as plt\n", "\n", "trs, _, _, labels = utils_files.load_ttest_dataset(utils_files.TTEST_KF_DS)\n", "\n", "tstat = ttest_computation(trs, labels)\n", "test_scale.display_ttest_result(tstat, ds1['traces'][0], title=\"k fixed ; pt different\")\n", "\n", "utils_plot.plot_temporal_average(\n", "    [trs[labels==0],trs[labels==1]],\n", "    [\"p0\", \"p1\"])"]}, {"cell_type": "markdown", "id": "b0efef5b-4bba-4f42-b204-8f117966b7e4", "metadata": {}, "source": ["### Going further\n", "\n", "Try to replicate the results on a \"cleaned\" dataset from which you have removed the traces with a significant shift in time average !"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.3"}}, "nbformat": 4, "nbformat_minor": 5}